Data replication

	Cassandra stores replicas on multiple nodes to ensure reliability and fault tolerance. 
	A replication strategy determines the nodes where replicas are placed. 
	The total number of replicas across the cluster is referred to as the replication factor. 
	A replication factor of 1 means that there is only one copy of each row in the cluster. 
	If the node containing the row goes down, the row cannot be retrieved. 
	A replication factor of 2 means two copies of each row, where each copy is on a different node. 
	All replicas are equally important; there is no primary or master replica. 
	As a general rule, the replication factor should not exceed the number of nodes in the cluster. 
	However, you can increase the replication factor and then add the desired number of nodes later.

	Two replication strategies are available:

		- SimpleStrategy: 
			Use only for a single datacenter and one rack. 
			If you ever intend more than one datacenter, use the NetworkTopologyStrategy.

		- NetworkTopologyStrategy: 
			Highly recommended for most deployments because it is much easier to expand to multiple datacenters 
			when required by future expansion.

	SimpleStrategy
		Use only for a single datacenter and one rack. 
		SimpleStrategy places the first replica on a node determined by the partitioner. 
		Additional replicas are placed on the next nodes clockwise in the ring without 
		considering topology (rack or datacenter location).

	NetworkTopologyStrategy
		Use NetworkTopologyStrategy when you have (or plan to have) your cluster deployed across multiple datacenters. 
		This strategy specifies how many replicas you want in each datacenter.
		
		NetworkTopologyStrategy places replicas in the same datacenter by walking the ring clockwise until reaching 
		the first node in another rack. 

		NetworkTopologyStrategy attempts to place replicas on distinct racks because nodes in the 
		same rack (or similar physical grouping) often fail at the same time due to power, cooling, or network issues.

	When deciding how many replicas to configure in each datacenter, the two primary considerations are 
	(1) being able to satisfy reads locally, without incurring cross data-center latency, and 
	(2) failure scenarios. 

	The two most common ways to configure multiple datacenter clusters are:
		- Two replicas in each datacenter: 
			This configuration tolerates the failure of a single node per replication group and 
			still allows local reads at a consistency level of ONE.

		- Three replicas in each datacenter: 
			This configuration tolerates either the failure of one node per replication group at a strong consistency level 
			of LOCAL_QUORUM or multiple node failures per datacenter using consistency level ONE.

	Asymmetrical replication groupings are also possible. 
	For example, you can have three replicas in one datacenter to serve real-time application requests 
	and use a single replica elsewhere for running analytics.

	Replication strategy is defined per keyspace, and is set during keyspace creation.

Data types

	Type 					Constant supported 			Description
	------------------------------------------------------------------------------------------------
	ascii					string 						ASCII character string
	bigint					integer						64-bit signed long
	blob					blob						Arbitrary bytes (no validation)
	boolean					boolean						Either true or false
	counter 				integer						Counter column (64-bit signed value). See counters for details.
	date 					integer, string 			A date (with no corresponding time value). See dates below for details.
	decimal					integer, float 				Variable-precision decimal
	double					integer, float 				64-bit IEEE-754 floating point
	duration 				duration 					A duration with nanosecond precision. See durations below for details.
	float 					integer, float 				32-bit IEEE-754 floating point
	inet 					string						An IP address, either IPv4 (4 bytes long) or IPv6 (16 bytes long). 
														Note that there is no inet constant, IP address should be input as strings.
	int 					integer						32-bit signed int
	smallint				integer 					16-bit signed int
	text					string 						UTF8 encoded string
	time 					integer, string 			A time (with no corresponding date value) with nanosecond precision. 
														See times below for details.
	timestamp 				integer, string 			A timestamp (date and time) with millisecond precision. 
														See timestamps below for details.
	timeuuid 				uuid 						Version 1 UUID, generally used as a “conflict-free” timestamp. 
														Also see timeuuid-functions.
	tinyint 				integer 					8-bit signed int
	uuid 					uuid 						A UUID (of any version)
	varchar 				string 						UTF8 encoded string
	varint 					integer 					Arbitrary-precision integer

	Counters
		The counter type is used to define counter columns. A counter column is a column whose value is a 64-bit signed integer and on which 2 operations are supported: incrementing and decrementing (see the UPDATE statement for syntax). 
		Note that the value of a counter cannot be set: a counter does not exist until first incremented/decremented, 
		and that first increment/decrement is made as if the prior value was 0.

		Counters have a number of important limitations:
			- They cannot be used for columns part of the PRIMARY KEY of a table.
			- A table that contains a counter can only contain counters. In other words, 
			  either all the columns of a table outside the PRIMARY KEY have the counter type, or none of them have it.
			- Counters do not support expiration.
			- The deletion of counters is supported, but is only guaranteed to work the first time you delete a counter. 
			  In other words, you should not re-update a counter that you have deleted 
			  (if you do, proper behavior is not guaranteed).
			- Counter updates are, by nature, not idemptotent. An important consequence is that if a 
			  counter update fails unexpectedly (timeout or loss of connection to the coordinator node), 
			  the client has no way to know if the update has been applied or not. 
			  In particular, replaying the update may or may not lead to an over count.


	Working with timestamps
		Values of the timestamp type are encoded as 64-bit signed integers representing a number of milliseconds since the standard base time known as the epoch: January 1 1970 at 00:00:00 GMT.
		
		Timestamps can be input in CQL either using their value as an integer, or using a string that represents an ISO 8601 date. For instance, all of the values below are valid timestamp values for Mar 2, 2011, at 04:05:00 AM, GMT:

		- 1299038700000
		- '2011-02-03 04:05+0000'
		- '2011-02-03 04:05:00+0000'
		- '2011-02-03 04:05:00.000+0000'
		- '2011-02-03T04:05+0000'
		- '2011-02-03T04:05:00+0000'
		- '2011-02-03T04:05:00.000+0000'

		The +0000 above is an RFC 822 4-digit time zone specification; +0000 refers to GMT. US Pacific Standard Time is -0800. 
		The time zone may be omitted if desired ('2011-02-03 04:05:00'), and if so, the date will be interpreted as being in the time zone under which the coordinating Cassandra node is configured. There are however difficulties inherent in relying on the time zone configuration being as expected, so it is recommended that the time zone always be specified for timestamps when feasible.
		
		The time of day may also be omitted ('2011-02-03' or '2011-02-03+0000'), in which case the time of day will default to 00:00:00 in the specified or default time zone. However, if only the date part is relevant, consider using the date type.


	Date type
		Values of the date type are encoded as 32-bit unsigned integers representing a number of days with “the epoch” at the center of the range (2^31). Epoch is January 1st, 1970

		For timestamps, a date can be input either as an integer or using a date string. In the later case, the format should be yyyy-mm-dd (so '2011-02-03' for instance).

	Time type
		Values of the time type are encoded as 64-bit signed integers representing the number of nanoseconds since midnight.
		
		For timestamps, a time can be input either as an integer or using a string representing the time. In the later case, the format should be hh:mm:ss[.fffffffff] (where the sub-second precision is optional and if provided, can be less than the nanosecond). So for instance, the following are valid inputs for a time:
		- '08:12:54'
		- '08:12:54.123'
		- '08:12:54.123456'
		- '08:12:54.123456789'

	Duration type
		Values of the duration type are encoded as 3 signed integer of variable lengths. 
		The first integer represents the number of months, the second the number of days and the third the number of nanoseconds. This is due to the fact that the number of days in a month can change, and a day can have 23 or 25 hours depending on the daylight saving. Internally, the number of months and days are decoded as 32 bits integers whereas the number of nanoseconds is decoded as a 64 bits integer.

		A duration can be input as:
			- (quantity unit)+ like 12h30m where the unit can be:
				- y: years (12 months)
				- mo: months (1 month)
				- w: weeks (7 days)
				- d: days (1 day)
				- h: hours (3,600,000,000,000 nanoseconds)
				- m: minutes (60,000,000,000 nanoseconds)
				- s: seconds (1,000,000,000 nanoseconds)
				- ms: milliseconds (1,000,000 nanoseconds)
				- us or µs : microseconds (1000 nanoseconds)
				- ns: nanoseconds (1 nanosecond)
			- ISO 8601 format: P[n]Y[n]M[n]DT[n]H[n]M[n]S or P[n]W
			- ISO 8601 alternative format: P[YYYY]-[MM]-[DD]T[hh]:[mm]:[ss]

		Duration columns cannot be used in a table’s PRIMARY KEY. 
		This limitation is due to the fact that durations cannot be ordered. 
		It is effectively not possible to know if 1mo is greater than 29d without a date context.
		A 1d duration is not equal to a 24h one as the duration type has been created to be able to support daylight saving.

	Collections
		CQL supports three kinds of collections: maps, sets and lists.
		Note however that neither bind_marker nor NULL are supported inside collection literals.

		Noteworthy characteristics
			Collections are meant for storing/denormalizing relatively small amount of data. 
			They work well for things like “the phone numbers of a given user”, “labels applied to an email”, etc. 
			But when items are expected to grow unbounded (“all messages sent by a user”, “events registered by a sensor”…​), then collections are not appropriate and a specific table (with clustering columns) should be used. Concretely, (non-frozen) collections have the following noteworthy characteristics and limitations:

			- Individual collections are not indexed internally. Which means that even to access a single element of 
			  a collection, the while collection has to be read (and reading one is not paged internally).
			- While insertion operations on sets and maps never incur a read-before-write internally, 
			  some operations on lists do. Further, some lists operations are not idempotent by nature 
			  (see the section on lists below for details), making their retry in case of timeout problematic. 
			  It is thus advised to prefer sets over lists when possible.

		Please note that while some of those limitations may or may not be removed/improved upon in the future, it is a anti-pattern to use a (single) collection to store large amounts of data.

		Maps
			A map is a (sorted) set of key-value pairs, where keys are unique and the map is sorted by its keys.

		Sets
			A set is a (sorted) collection of unique values. 

		Lists
			A list is a (sorted) collection of non-unique values where elements are ordered by there position in the list. 

	User-Defined Types (UDTs)
		CQL support the definition of user-defined types (UDTs). Such a type can be created, modified and removed using the create_type_statement, alter_type_statement and drop_type_statement described below. But once created, a UDT is simply referred to by its name:
			user_defined_type::= udt_name
			udt_name::= [ keyspace_name '.' ] identifier

		Things to keep in mind about UDTs:
			
			- Attempting to create an already existing type will result in an error unless the IF NOT EXISTS option is used. 
			  If it is used, the statement will be a no-op if the type already exists.
			
			- A type is intrinsically bound to the keyspace in which it is created, and can only be used in that keyspace. 
			  At creation, if the type name is prefixed by a keyspace name, it is created in that keyspace. 
			  Otherwise, it is created in the current keyspace.
			
			- As of Cassandra , UDT have to be frozen in most cases, hence the frozen<address> in the table definition above.
			  Please see the section on frozen for more details.

	Tuples
		CQL also support tuples and tuple types (where the elements can be of different types). 
		Functionally, tuples can be though as anonymous UDT with anonymous fields.

		Unlike other composed types, like collections and UDTs, a tuple is always frozen <frozen> 
		(without the need of the frozen keyword) and it is not possible to update only some elements of a tuple 
		(without updating the whole tuple). Also, a tuple literal should always have the same number of value 
		than declared in the type it is a tuple of (some of those values can be null but they need to be explicitly declared as so).

	Custom Types
		A custom type is defined by:
			custom_type::= string

		A custom type is a string that contains the name of Java class that extends the server side AbstractType class and that can be loaded by Cassandra (it should thus be in the CLASSPATH of every node running Cassandra). That class will define what values are valid for the type and how the time sorts when used for a clustering column. For any other purpose, a value of a custom type is the same than that of a blob, and can in particular be input using the blob literal syntax.

The Primary key
	Within a table, a row is uniquely identified by its PRIMARY KEY, and hence all tables must define a single PRIMARY KEY. 
	A PRIMARY KEY is composed of one or more of the defined columns in the table. 
	Syntactically, the primary key is defined with the phrase PRIMARY KEY followed by a comma-separated list of the column names within parenthesis. If the primary key has only one column, you can alternatively add the PRIMARY KEY phrase to that column in the table definition. The order of the columns in the primary key definition defines the partition key and clustering columns.

	A CQL primary key is composed of two parts:
		- partition key
			It is the first component of the primary key definition. It can be a single column or, using an additional set of parenthesis, can be multiple columns. A table must have at least one partition key, the smallest possible table definition is:

			CREATE TABLE t (k text PRIMARY KEY);
		
		- clustering columns
			The columns are the columns that follow the partition key in the primary key definition. The order of those columns define the clustering order.

	Some examples of primary key definition are:
		- PRIMARY KEY (a): a is the single partition key and there are no clustering columns
		
		- PRIMARY KEY (a, b, c) : a is the single partition key and b and c are the clustering columns

		- PRIMARY KEY ((a, b), c) : a and b compose the composite partition key and c is the clustering column

		Important: 
			The primary key uniquely identifies a row in the table, as described above. 
			A consequence of this uniqueness is that if another row is inserted using the same primary key, 
			then an UPSERT occurs and an existing row with the same primary key is replaced. 
			Columns that are not part of the primary key cannot define uniqueness.

	Partition key
		Within a table, CQL defines the notion of a partition that defines the location of data within a Cassandra cluster. 
		A partition is the set of rows that share the same value for their partition key.

		Note that if the partition key is composed of multiple columns, then rows belong to the same partition when they have the same values for all those partition key columns. A hash is computed from the partition key columns and that hash value defines the partition location. So, for instance, given the following table definition and content:
			
			CREATE TABLE t (
    			a int,
    			b int,
			    c int,
			    d int,
			    PRIMARY KEY ((a, b), c, d)
			);
			
			INSERT INTO t (a, b, c, d) VALUES (0,0,0,0);
			INSERT INTO t (a, b, c, d) VALUES (0,0,1,1);
			INSERT INTO t (a, b, c, d) VALUES (0,1,2,2);
			INSERT INTO t (a, b, c, d) VALUES (0,1,3,3);
			INSERT INTO t (a, b, c, d) VALUES (1,1,4,4);
			SELECT * FROM t;

		will result in
			 a | b | c | d
			---+---+---+---
			 0 | 0 | 0 | 0 	
			 0 | 0 | 1 | 1
			 0 | 1 | 2 | 2	
			 0 | 1 | 3 | 3
			 1 | 1 | 4 | 4  

			(5 rows)

		- Rows 1 and 2 are in the same partition, because both columns a and b are zero.
		- Rows 3 and 4 are in the same partition, but a different one, because column a is zero and column b is 1 in both rows.
		- Row 5 is in a third partition by itself, because both columns a and b are 1.

		Note that a table always has a partition key, and that if the table has no clustering columns, then every partition of that table has a single row. because the partition key, compound or otherwise, identifies a single location.

		The most important property of partition is that all the rows belonging to the same partition are guaranteed to be stored on the same set of replica nodes. In other words, the partition key of a table defines which rows will be localized on the same node in the cluster. 
		The localization of data is important to the efficient retrieval of data, requiring the Cassandra coordinator to contact as few nodes as possible. However, there is a flip-side to this guarantee, and all rows sharing a partition key will be stored on the same node, creating a hotspot for both reading and writing. While selecting a primary key that groups table rows assists batch updates and can ensure that the updates are atomic and done in isolation, the partitions must be sized "just right, not too big nor too small".

		Data modeling that considers the querying patterns and assigns primary keys based on the queries will have the lowest latency in fetching data.

	Clustering columns
		The clustering columns of a table define the clustering order for the partition of that table. For a given partition, all rows are ordered by that clustering order. Clustering columns also add uniqueness to a row in a table.
		For instance, given:

		CREATE TABLE t2 (
		    a int,
		    b int,
		    c int,
		    d int,
		    PRIMARY KEY (a, b, c)
		);

		INSERT INTO t2 (a, b, c, d) VALUES (0,0,0,0);
		INSERT INTO t2 (a, b, c, d) VALUES (0,0,1,1);
		INSERT INTO t2 (a, b, c, d) VALUES (0,1,2,2);
		INSERT INTO t2 (a, b, c, d) VALUES (0,1,3,3);
		INSERT INTO t2 (a, b, c, d) VALUES (1,1,4,4);
		SELECT * FROM t2;

		will result in

		 a | b | c | d
		---+---+---+---
		 1 | 1 | 4 | 4	
		 0 | 0 | 0 | 0
		 0 | 0 | 1 | 1
		 0 | 1 | 2 | 2
		 0 | 1 | 3 | 3

		(5 rows)

		Row 1 is in one partition, and Rows 2-5 are in a different one. The display order is also different.

		Looking more closely at the four rows in the same partition, the b clustering column defines the order in which those rows are displayed. Whereas the partition key of the table groups rows on the same node, the clustering columns control how those rows are stored on the node.

		That sorting allows the very efficient retrieval of a range of rows within a partition:
			
			SELECT * FROM t2 WHERE a = 0 AND b > 0 and b <= 3;
		
		will result in
			 a | b | c | d
			---+---+---+---
			 0 | 1 | 2 | 2
			 0 | 1 | 3 | 3

			(2 rows)

	Impacts of data partition on Cassandra clusters
		Controlling the size of the data stored in each partition is essential to ensure even distribution of data across the cluster and to get good I/O performance. Below are the impacts Partitioning has on some of the different aspects of a Cassandra cluster:

		- Read Performance
			Cassandra maintains caches, indexes and index summaries to locate partitions within SSTables files on disk. Large partitions cause inefficiency in maintaining these data structures and result in performance degradation. The Cassandra project has made several improvements in this area, especially in version 3.6 where the engine was restructured to be more performant for large partitions and more resilient against memory issues and crashing.

		- Memory Usage
			The partition size directly impacts the JVM heap size and garbage collection mechanism. Large partitions increase pressure on the JVM heap and make garbage collection inefficient.

		- Cassandra Repairs
			Repair is a maintenance operation to make data consistent. It involves scanning data and comparing it with other data replicas followed by data streaming if required. Large partition sizes make it hard to repair data.

		- Tombstones Eviction
			Cassandra uses unique markers called ‘tombstones’ to mark data deletion. Large partitions can contribute to difficulties in tombstone eviction if data deletion pattern and compaction strategy are not appropriately implemented. 

		Being aware of these impacts helps in an optimal partition key design while deploying Cassandra. It might be tempting to design the partition key to having only one row or a few rows per partition. However, a few other factors might influence the design decision, primarily, the data access pattern and ideal partition size.

	The important elements of the Cassandra partition key discussion are summarized below:
		1. Each Cassandra table has a partition key which can be standalone or composite. 
		2. The partition key determines data locality through indexing in Cassandra.
		3. The partition size is a crucial attribute for Cassandra performance and maintenance.
		4. The ideal size of a Cassandra partition is equal to or lower than 10MB with a maximum of 100MB.
		5. The partition key should be designed carefully to create bounded partitions with size in the ideal range. 
		6. It is essential to understand your data demographics and consider partition size and data distribution when designing your schema. There are several tools to test, analyse and monitor Cassandra partitions.
		7. The Cassandra version 3.6 and above incorporates significant improvements in the storage engine which provides much better partition handling.

Tombstones
	Cassandra generates tombstones when you delete data. Under some circumstances, excess tombstones can cause long GC pauses, latency, read failures, or out of heap errors. This article provides advice for managing tombstones.

	What is a tombstone?
		In Cassandra, deleted data is not immediately purged from the disk. Instead, Cassandra writes a special value, known as a tombstone, to indicate that data has been deleted. Tombstones prevent deleted data from being returned during reads, and will eventually allow the data to be dropped via compaction.

		Tombstones are writes – they go through the normal write path, take up space on disk, and make use of Cassandra’s consistency mechanisms. Tombstones can be propagated across the cluster via hints and repairs. If a cluster is managed properly, this ensures that data will remain deleted even if a node is down when the delete is issued.

		Tombstones are generated by:
		- DELETE statements
		- Setting TTLs
		- Inserting null values
		- Inserting data into parts of a collection.

	What is the normal lifecycle of tombstones?
		Tombstones are written with a timestamp. Under ideal circumstances, tombstones (and their associated data) will be dropped during compactions after a certain amount of time has passed.

		The following three criteria must be met for tombstones to be removed:
		- The tombstones were created more than gc_grace_seconds ago.
		- The table containing the tombstone is involved in a compaction.
		- All sstables that could contain the relevant data are involved in the compaction.

		Each table has a gc_grace_seconds setting. By default, this is set to 864000, which is equivalent to 10 days. 
		The intention is to provide time for the cluster to achieve consistency via repairs 
		(and hence, prevent the resurrection of deleted data).

		Tombstones will only be dropped via a compaction if all sstables that could contain the relevant data 
		are involved in the compaction. If a lot of time has elapsed between writing the original data and issuing the DELETE, 
		this becomes less likely:
			
			- Size-Tiered Compaction Strategy 
				will compact sstables of similar size together. Data tends to move into larger sstables as it ages, so the tombstone (in a new, small sstable) is unlikely to be compacted with the data (in an old, large sstable).
			
			- Leveled Compaction Strategy 
				is split into many levels that are compacted separately. The tombstone will be written into level 0 and will effectively ‘chase’ the data through the levels – it should eventually catch up.

			- Time-Window Compaction Strategy (or Date-Tiered Compaction Strategy) 
				will never compact the tombstone with the data if they are written into different time windows.

	When do tombstones cause problems?
		
		Disk usage
			When data is deleted, the space will not actually be freed for at least the gc_grace period set in the table settings. This can cause problems if a cluster is rapidly filling up.
			Under some circumstances, the space will never be freed without manual intervention.

		Read performance
			Serious performance problems can occur if reads encounter large numbers of tombstones.
			
			Performance issues are most likely to happen with the following types of query:
				- Queries that run over all partitions in a table (“select * from keyspace.table”)
				- Range queries (“select * from keyspace.table WHERE value > x”, or “WHERE value IN (value1, value2, …)”
				- Any query that can only be run with an “ALLOW FILTERING” clause.

			These performance issues occur because of the behaviour of tombstones during reads. In a range query, your Cassandra driver will normally use paging, which allows nodes to return a limited number of responses at a time. When cassandra tombstones are involved, the node needs to keep the tombstones that it has encountered in memory and return them to the coordinator, in case one of the other replicas is unaware that the relevant data has been deleted. The tombstones cannot be paged because it is essential to return all of them, so latency and memory use increase proportionally to the number of tombstones encountered.

			Whether the tombstones will be encountered depends on the way the data is stored and retrieved. For example, if Cassandra is used to store data in a queue (which is not recommended), queries may encounter tens of thousands of tombstones to return a few rows of data.

	How can I diagnose tombstone-related problems?

		
















