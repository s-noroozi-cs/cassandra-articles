eBay
	We’ve been trying out Cassandra for more than a year(2010-2011). 
	Cassandra is now serving a handful of use cases ranging from write-heavy logging and tracking, to mixed workload. 
	One of them serves our “Social Signal” project, which enables like/own/want features on eBay product pages. 
	A few use cases have reached production, while more are in development.

	Our Cassandra deployment is not huge, but it’s growing at a healthy pace. 
	In the past couple of months, we’ve deployed dozens of nodes across several small clusters spanning multiple data centers. 
	You may ask, why multiple clusters? 
		We isolate clusters by functional area and criticality. 
		Use cases with similar criticality from the same functional area share the same cluster, 
		but reside in different keyspaces.

	RedLaser, Hunch, and other eBay adjacencies are also trying out Cassandra for various purposes. 
	In addition to Cassandra, we also utilize MongoDB and HBase. 
	I won’t discuss these now, but suffice it to say we believe each has its own merit.
	
	
	Marketplaces
		97 Million active buyers and sellers
		200+ Million items
		2 billion page views each day
		80 billion database calls each day
		5+ petabytes of site storage capacity
		80+ petabytes of analytics storage capacity
		
	Use cases
		Social Signals on eBay product & item pages
		Hunch taste graph for eBay users & items
		Time series use cases (many): 
			Mobile notification logging and tracking
			Tracking for fraud detection
			SOA request/response payload logging
			RedLaser server logs and analytics
			
Netflix
	
	Distributed Tracing Infrastructure
	
		We started with Elasticsearch as our data store due to its flexible data model and querying capabilities. 
		As we onboarded more streaming services, the trace data volume started increasing exponentially. 
		The increased operational burden of scaling ElasticSearch clusters due to high data write rate became painful for us. 
		The data read queries took an increasingly longer time to finish because 
		ElasticSearch clusters were using heavy compute resources for creating indexes on ingested traces. 
		The high data ingestion rate eventually degraded both read and write operations. 
		We solved this by migrating to Cassandra as our data store for handling high data ingestion rates. 
		Using simple lookup indices in Cassandra gives us the ability to maintain acceptable read latencies while doing heavy writes.

		In theory, scaling up horizontally would allow us to handle higher write rates and retain larger amounts of data in Cassandra clusters. 
		This implies that the cost of storing traces grows linearly to the amount of data being stored. 
		We needed to ensure storage cost growth was sub-linear to the amount of data being stored. 
		In pursuit of this goal, we outlined following storage optimization strategies:

			1. Use cheaper Elastic Block Store (EBS) volumes instead of SSD instance stores in EC2.
			2. Employ better compression technique to reduce trace data size.
			3. Store only relevant and interesting traces by using simple rules-based filters.

		We were adding new Cassandra nodes whenever the EC2 SSD instance stores of existing nodes reached maximum storage capacity. 
		The use of a cheaper EBS Elastic volume instead of an SSD instance store was an attractive option 
		because AWS allows dynamic increase in EBS volume size without re-provisioning the EC2 node. 
		This allowed us to increase total storage capacity without adding a new Cassandra node to the existing cluster. 
		
		In 2019 our stunning colleagues in the Cloud Database Engineering (CDE) team benchmarked EBS performance for our use case 
		and migrated existing clusters to use EBS Elastic volumes. By optimizing the Time Window Compaction Strategy (TWCS) parameters, 
		they reduced the disk write and merge operations of Cassandra SSTable files, thereby reducing the EBS I/O rate. 
		This optimization helped us reduce the data replication network traffic amongst the cluster nodes because 
		SSTable files were created less often than in our previous configuration. 
		
		Additionally, by enabling Zstd block compression on Cassandra data files, the size of our trace data files was reduced by half. 
		With these optimized Cassandra clusters in place, 
		it now costs us 71% less to operate clusters and we could store 35x more data than our previous configuration.
		
Instagram
	
	At Instagram, we use Apache Cassandra heavily as a general key value storage service. 
	The majority of Instagram’s Cassandra requests are online, 
	so in order to provide a reliable and responsive user experience for hundreds of millions of Instagram users, 
	we have very tight SLA on the metrics. 
	
	Instagram maintains a 5–9s reliability SLA, which means at any given time, the request failure rate should be less than 0.001%. 
	For performance, we actively monitor the throughput and latency of different Cassandra clusters, especially the P99 read latency. 
	
	After investigation, we found the JVM garbage collector (GC) contributed a lot to the latency spikes. 
	We defined a metric called GC stall percentage to measure the percentage of time a Cassandra server was doing stop-the-world GC (Young Gen GC) 
	and could not serve client requests.
	
	The graph shows that a Cassandra server instance could spend 2.5% of runtime on garbage collections instead of serving client requests. 
	The GC overhead obviously had a big impact on our P99 latency, so if we could lower the GC stall percentage, 
	we would be able to reduce our P99 latency significantly.
	
	Apache Cassandra is a distributed database with it’s own LSM tree-based storage engine written in Java. 
	We found that the components in the storage engine, like memtable, compaction, read/write path, etc., 
	created a lot of objects in the Java heap and generated a lot of overhead to JVM. 
	To reduce the GC impact from the storage engine, 
	we considered different approaches and ultimately decided to develop a C++ storage engine to replace existing ones.
	
	We did not want to build a new storage engine from scratch, so we decided to build the new storage engine on top of RocksDB. 
	
	RocksDB is an open source, high-performance embedded database for key-value data. 
	It’s written in C++, and provides official API language bindings for C++, C, and Java. 
	RocksDB is optimized for performance, especially on fast storage like SSD. 
	It’s widely used in the industry as the storage engine for MySQL, mongoDB, and other popular databases.

	The first challenge was that Cassandra does not have a pluggable storage engine architecture yet, 
	which means the existing storage engine is coupled together with other components in the database. 
	To find a balance between massive refactoring and quick iterations, we defined a new storage engine API, 
	including the most common read/write and streaming interfaces. 
	This way we could implement the new storage engine behind the API and inject it into the related code paths inside Cassandra.
	
	Secondly, Cassandra supports rich data types and table schema, while RocksDB provides purely key-value interfaces. 
	We carefully defined the encoding/decoding algorithms to support Cassandra’s data model within RocksDB’s data structure and 
	supported same-query semantics as original Cassandra.
	
	The third challenge was about streaming. Streaming is an important component for a distributed database like Cassandra. 
	Whenever we join or remove a node from a Cassandra cluster, Cassandra needs to stream data among different nodes to balance the load across the cluster. 
	The existing streaming implementation was based on the details in the current storage engine. 
	Accordingly, we had to decouple them from each other, make an abstraction layer, and re-implement the streaming using RocksDB APIs. 
	For high streaming throughput, we now stream data into temp sst files first, and then use the RocksDB ingest file API to bulk load them 
	into the RocksDB instance at once.
	
	After about a year of development and testing, we have finished a first version of the implementation and 
	successfully rolled it into several production Cassandra clusters in Instagram. 
	In one of our production clusters, the P99 read latency dropped from 60ms to 20ms. 
	We also observed that the GC stalls on that cluster dropped from 2.5% to 0.3%, which was a 10X reduction!
	
	We have open sourced our Rocksandra code base and benchmark framework, which you can download from Github to try out in your own environment! 
	https://github.com/Instagram/cassandra/tree/rocks_3.0
	https://github.com/Instagram/cassandra-aws-benchmark

Spotify
	
	Personalization at Spotify using Cassandra

	At Spotify we have have over 60 million active users who have access to a vast music catalog of over 30 million songs. 
	Our users have a choice to follow thousands of artists and hundreds of their friends and create their own music graph. 
	On our service they also discover new and existing content by experiencing a variety of music promotions 
	(album releases, artist promos), which get served over our ad platform. 
	These options have empowered our users and made them really engaged. 
	Over time they have created over 1.5 billion playlists and just last year they streamed over 7 billion hours worth of music.

	But at times an abundance of options has also made our users feel a bit lost. 
	How do you find that right playlist for your workout from over a billion playlists? 
	How do you discover new albums which are relevant to your taste? 
	We help our users discover and experience relevant content by personalizing their experience on our platform.

	Personalizing user experience involves learning their tastes and distastes in different contexts.
	A metal genre listener might not enjoy an announcement for a metal genre album when they are trying to put 
	their kid to sleep and playing kid’s music at night. 
	Serving them a recommendation for a kid’s music album might be more relevant in that context. 
	But this experience might not be relevant for another metal genre listener who doesn’t mind 
	receiving metal genre album recommendations during any context. 
	These two users with similar listening habits might have different preferences. 
	Personalizing their experiences on Spotify according to their respective taste in different contexts helps us make them more engaged.
	
	Overall Architecture
		In our personalization stack we are using Kafka for log collection, Storm for real-time event processing, 
		Crunch for running batch map-reduce jobs on Hadoop and Cassandra to store user profile attributes 
		and metadata about entities like playlists, artists, etc.

		In the diagram below logs are forwarded by Kafka producers, running on different services and emitting different kinds of events like completion of a song and delivery of an ad impression, to the Kafka broker. There are 2 sets of Kafka consumers, subscribed to different topics, for consuming events:

		Kafka Consumers for Hadoop which write these events to HDFS. All the raw logs on HDFS are then processed in Crunch to remove duplicate events, filter out unwanted fields and convert records into Avro Format.

		Kafka Consumer Spouts running within Storm topologies which stream the events for real-time computation.
		
		There are also other Crunch pipelines which ingest and generate metadata (genre, tempo, etc.) for different entities. These data records are stored in HDFS and also exported using  Crunch to Cassandra for real-time lookup in Storm pipelines. We will call the Cassandra cluster which stores entity metadata the Entity Metadata Store (EMS).

		The Storm pipelines process raw log events from Kafka, filter out unwanted events, decorate the entities with metadata fetched from EMS, group it per user and determine user level attributes by some algorithmic combination of aggregation and derivation. These user attributes when combined represent a user’s profile and they are stored in a Cassandra Cluster which we will call the User Profile Store (UPS). [spotify-cassandra-personalization.png]

	Why is Cassandra a good fit?
		Since UPS is central to our personalization system, in this post we will elaborate why we chose Cassandra for storage. 
		When we started shopping around for different storage solutions for UPS we desired a solution which could:

		- Scale horizontally
		- Support replication – preferably cross-site
		- Have low latency even at the cost of consistency since we aren’t performing transactions
		- Have the ability to load bulk and streaming data from Crunch and Storm respectively
		- Have a decent ability to model different data schemas for different use-cases of entity metadata since we didn’t want to invest in yet another solution for EMS as that would have increased our operational cost.

		We considered variety of solutions that we commonly use at Spotify like Memcached, Sparkey and Cassandra. Only Cassandra fit the bill for all these requirements.

Common Apache Cassandra Use Cases
	
	Cassandra's benefits include:
		- Open source
			Increases innovation, speed of implementation, flexibility, and extensibility. 
			More cost effective, while avoiding vendor lock-in.

		- Handles a high volume of data with ease
			Built to handle a massive amount of data across many servers. 
			Some large organizations are using it to manage petabytes of information.

		- Continuous availability
			No single point of failure means zero downtime. 
			If a particular node fails, users will be automatically moved to the closest working node. 
			The system will continue to work as designed, with applications always available, and data always accessible. 
			Users will never know there was an outage. 
			This is a key for companies that can’t afford to ever have their database go offline, or to lose any data.

		- High performance and fast
			Cassandra has a peer-to-peer, distributed architecture, where every node can perform all read and write operations. This adds resiliency, while improving performance. Write speed is especially fast. 
			And Cassandra can write loads of data, without speed or accuracy being affected.

		- Straightforward scalability
			Cassandra’s horizontal scaling is straightforward and cost-effective. 
			Instead of scaling vertically with expensive hardware, Cassandra enables companies to expand to any size simply by adding low-cost commodity servers or virtual machines—no shutdowns required. 
			And its linear scalability ensures high performance is maintained across all nodes. 
			Cassandra’s scalability benefits make it popular with companies working with large datasets, 
			that have many concurrent users, and are expecting continued growth.

		- Seamless replication
			Like other NoSQL databases, Cassandra doesn’t require a fixed schema, making replication simple. 
			And, since it’s a peer-to-peer system, data can be quickly replicated across the entire system, 
			regardless of geographic location. Wide, even global, distribution is possible. 
			Replication across data centers creates high fault tolerance and zero data loss—an outage 
			in any particular region won’t matter. And placing data closer to end users also leads to low latency.

		- Familiar Interface
			Most developers will be able to pick up Cassandra’s query language quickly. 
			That’s because Cassandra Query Language (CQL) has a strong resemblance to SQL.
	
	Sure, Cassandra has a laundry list of benefits, but how is it actually being used to help companies? On the community team at DataStax we spend a lot of time talking to, and hearing from, companies that are using Apache Cassandra in production.
	
	Here are some use cases we see often:
		
		- E-commerce and inventory management
		
		- Personalization, recommendations, and customer experience
		
		- Internet of things and edge computing
		
		- Fraud detection and authentication


	E-commerce and inventory management
		
		E-commerce companies can’t afford to have their site go down, and that’s especially true during a peak period. 
		Every minute they’re offline quickly eats away at their bottom line. 
		Since rapid growth is always the goal, they also need the ability to cost-effectively scale their online inventory 
		on the fly. For the same reason, these organizations need a database that can handle an enormous amount of data with ease. And to meet or exceed customer expectations, they need the flexibility to continuously modify their product mix.

		Here’s why Cassandra is a good fit for e-commerce and inventory management:

			- Resilient with zero downtime
				Distributed with multi-region replication, Cassandra ensures zero downtime. 
				Even the loss of an entire region won’t bring it down.
			
			- Highly responsive
				Cassandra’s peer-to-peer architecture also allows data to reside in regions around the world and closer to any particular customer—allowing the system to be highly responsive and fast.

			- Predictable scalability
				Cassandra’s horizontal scalability is straightforward, predictable, and cost-effective.
		
			- Provides faster catalog refreshes
			
			- Analyzes its catalog and inventory in real time. 

	Personalization, recommendations, and customer experience

		Personalization and recommendation engines are everywhere now. Almost like personal assistants built-into apps and websites, they help us decide what events to buy tickets to, surfacing articles we might find interesting, and much more. Eventbrite now uses Cassandra instead of MySQL to power their mobile experience, letting users know what events are happening around them that they will be interested in attending. Eventbrite chose Cassandra for its read/write capacity and ease of deployment. Outbrain, a company you use frequently, but may be unfamiliar with, uses Cassandra to power their content discovery platform, helping companies add revenue streams by serving up applicable third-party articles you may find interesting. 

		Near real-time, relevant, personalized experiences are now expected. Here’s why Cassandra is the right choice to power tailored experiences:

			- Fast response times.
			- Extremely low latency, even as your customer base expands.
			- Handles all types of data, structured and unstructured, from a variety of sources.
			- Built to scale while staying cost-effective.
			- Ability to store, manage, query, and modify extremely large datasets, 
			  while delivering personalized experiences to millions of customers.
			- High read/write capacity.
			- Ease of deployment.
			- Flexible, enabling continuous customer experience innovation.

		Consider the success story of Macquarie Bank. With an architectural foundation built on Cassandra, the company moved from no retail banking presence to a top contender in the digital banking space in less than two years, by truly understanding customer behavior and prioritizing personalization. Learn how MacQuarie Bank uses Cassandra to provide personalization for their customers.

			https://www.datastax.com/enterprise-success/macquarie-bank/achieving-digital-transformation-with-dse

	Internet of things and edge computing

		Whether tracking weather, traffic, energy consumption, inventory levels, health indicators, video game stats, farming conditions or countless other metrics, internet of things (IoT) sensors, wearables, vehicles, mobile devices, appliances, drones, and other devices at the edge produce an avalanche of never-ending data. This data needs to be securely collected—sometimes from millions of devices—aggregated, processed, and analyzed on an ongoing basis.

		Consider how the National Renewable Energy Laboratory uses Cassandra to store and analyze sensor data at the world's most environmentally friendly building. They find ways to save water and energy by running the world's smartest thermostat on top of Cassandra. The system continuously learns about energy usage patterns, and automatically adjusts settings, even when no one is there to program it.

		Here are some of the reasons Cassandra is a good fit for IoT and edge computing needs:

			- Cassandra can ingest concurrent data from any node in the cluster, since all have read/write capacity.
			- Ability to handle a large volume of high-velocity, time-series data.
			- High availability.
			- Supports continuous, real-time analysis.

	Fraud detection and authentication

		Security threats continue to rise, and many companies are always on the defensive, playing catch-up with their smart fraud detection capabilities. That’s because fraudsters are constantly on the attack, looking for new and creative ways to steal customer data and compromise other sensitive information.

		To have any chance of preventing illegitimate users from gaining access, companies need data and a lot of it. Continuous, real-time analysis of large and diverse datasets is required to find patterns and anomalies that can be indicators of fraud. A high priority for all businesses, fraud detection’s importance is elevated in areas like financial services, banking, payments, and insurance. As an example, take a look at how ACI Worldwide has used Cassandra to drastically improve its fraud detection rate and false positive rate.

		Identity authentication is the other side of the fraud detection coin. Instead of focusing on keeping fraudsters out, the goal of authentication is to confirm that only legitimate customers gain access. The trick is, you want to make the log-in process as painless and fast as possible, while still making absolutely sure they are who they say they are. As with fraud detection, to pull this off, you need to conduct real-time analysis of a wide variety and high volume of data. And since authentication is likely a central part of all your systems, outages must be avoided at all costs. If a customer experiences friction trying to access your site, whether due to a false positive or because the auth system is down, it likely won’t take too long for them to leave in frustration.

		Here are some reasons Cassandra is a great database choice for fighting fraud and ensuring identity authentication:

			- Flexible schema
				Handles numerous data types, and they can be quickly added to the mix.
			- Enables complex, real-time analysis, including the ability to incorporate and support machine learning and AI.
			- Handles large-scale, growing datasets.

	Other common Cassandra use cases

		There are countless other applications that can benefit from Cassandra. Here are a few more:

			- Financial services and payments
			- Messaging
			- Playlists
			- Logistics and asset management
			- Content management systems
			- Transaction logging
			- Tracking of all kinds, including packages and orders
			- Digital and media management

	Cassandra provides a solid foundation for dashboards for many reasons, including:

		- Easily handles frequent updates—there are typically many, ongoing updates for each user.
		- Built to take on extremely large datasets—hundreds of millions of events can reside in one table.
		- Efficient way to store time-series data.

		Soundcloud

			The dashboard Soundcloud provides its customers is one of its most popular features. In fact, the company credits much of its rapid data growth to it. Soundcloud customers can personalize their dashboard with the option to see where in the world their audio uploads are being listened to, and by which users, along with being a home for incoming sound clips from people they follow, and much more.

			Soundcloud turned to Cassandra because of its ability to store and access vast amounts of data, its built-in persistence of that data, and for its scalability. Cassandra’s read/write capabilities were also a strong selling point for Soundcloud’s adoption. With Cassandra, they can provide each customer with a sequential read path—so posts can be browsed in the correct time order. Cassandra also allows Soundcloud users to randomly access write events, and have any particular one put into sequential order. One write event could end up in millions of users’ dashboards, and Soundcloud uses Cassandra to ensure it's always displayed in the right place. The company also leans on Cassandra to explore relationships between customers, and personalize their experiences.

	Replication

		As discussed earlier, creating and storing replicas of datasets at geographically dispersed data centers makes a lot of sense. It increases fault tolerance, reliability, and availability. If a data center in the cluster goes down, operations will continue without a blip. Having data closer to customers, no matter where they’re using your app around the world, also decreases read and write latency.

		Cassandra has a peer-to-peer, distributed architecture, without the need of a primary node. Instead, every node can perform read and write operations, and all replicas, across the cluster, are equally important. That means data can quickly be replicated across all nodes and Cassandra doesn’t have a single point of failure. That translates into always-on availability with zero downtime. That’s why so many companies turn to Cassandra when data storage is mission critical, and they need a database that can comfortably handle petabyte-sized datasets and full global replication.

		Instagram

			Instagram has used Cassandra from the beginning, way back in 2010. When they started expanding, they created replicas in each new data center. As Instagram kept replicating in data centers around the world, they found their performance dropped. To combat the dip, they started storing data only in the region closest to where it was generated. Local data access has helped them provide a faster, more efficient service to the more than one billion active daily users they have today. Learn more about how Instagram uses Cassandra to replicate on a global scale.

	