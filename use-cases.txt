eBay
	We’ve been trying out Cassandra for more than a year(2010-2011). 
	Cassandra is now serving a handful of use cases ranging from write-heavy logging and tracking, to mixed workload. 
	One of them serves our “Social Signal” project, which enables like/own/want features on eBay product pages. 
	A few use cases have reached production, while more are in development.

	Our Cassandra deployment is not huge, but it’s growing at a healthy pace. 
	In the past couple of months, we’ve deployed dozens of nodes across several small clusters spanning multiple data centers. 
	You may ask, why multiple clusters? 
		We isolate clusters by functional area and criticality. 
		Use cases with similar criticality from the same functional area share the same cluster, 
		but reside in different keyspaces.

	RedLaser, Hunch, and other eBay adjacencies are also trying out Cassandra for various purposes. 
	In addition to Cassandra, we also utilize MongoDB and HBase. 
	I won’t discuss these now, but suffice it to say we believe each has its own merit.
	
	
	Marketplaces
		97 Million active buyers and sellers
		200+ Million items
		2 billion page views each day
		80 billion database calls each day
		5+ petabytes of site storage capacity
		80+ petabytes of analytics storage capacity
		
	Use cases
		Social Signals on eBay product & item pages
		Hunch taste graph for eBay users & items
		Time series use cases (many): 
			Mobile notification logging and tracking
			Tracking for fraud detection
			SOA request/response payload logging
			RedLaser server logs and analytics
			
Netflix
	
	Distributed Tracing Infrastructure
	
		We started with Elasticsearch as our data store due to its flexible data model and querying capabilities. 
		As we onboarded more streaming services, the trace data volume started increasing exponentially. 
		The increased operational burden of scaling ElasticSearch clusters due to high data write rate became painful for us. 
		The data read queries took an increasingly longer time to finish because 
		ElasticSearch clusters were using heavy compute resources for creating indexes on ingested traces. 
		The high data ingestion rate eventually degraded both read and write operations. 
		We solved this by migrating to Cassandra as our data store for handling high data ingestion rates. 
		Using simple lookup indices in Cassandra gives us the ability to maintain acceptable read latencies while doing heavy writes.

		In theory, scaling up horizontally would allow us to handle higher write rates and retain larger amounts of data in Cassandra clusters. 
		This implies that the cost of storing traces grows linearly to the amount of data being stored. 
		We needed to ensure storage cost growth was sub-linear to the amount of data being stored. 
		In pursuit of this goal, we outlined following storage optimization strategies:

			1. Use cheaper Elastic Block Store (EBS) volumes instead of SSD instance stores in EC2.
			2. Employ better compression technique to reduce trace data size.
			3. Store only relevant and interesting traces by using simple rules-based filters.

		We were adding new Cassandra nodes whenever the EC2 SSD instance stores of existing nodes reached maximum storage capacity. 
		The use of a cheaper EBS Elastic volume instead of an SSD instance store was an attractive option 
		because AWS allows dynamic increase in EBS volume size without re-provisioning the EC2 node. 
		This allowed us to increase total storage capacity without adding a new Cassandra node to the existing cluster. 
		
		In 2019 our stunning colleagues in the Cloud Database Engineering (CDE) team benchmarked EBS performance for our use case 
		and migrated existing clusters to use EBS Elastic volumes. By optimizing the Time Window Compaction Strategy (TWCS) parameters, 
		they reduced the disk write and merge operations of Cassandra SSTable files, thereby reducing the EBS I/O rate. 
		This optimization helped us reduce the data replication network traffic amongst the cluster nodes because 
		SSTable files were created less often than in our previous configuration. 
		
		Additionally, by enabling Zstd block compression on Cassandra data files, the size of our trace data files was reduced by half. 
		With these optimized Cassandra clusters in place, 
		it now costs us 71% less to operate clusters and we could store 35x more data than our previous configuration.
		
Instagram
	
	At Instagram, we use Apache Cassandra heavily as a general key value storage service. 
	The majority of Instagram’s Cassandra requests are online, 
	so in order to provide a reliable and responsive user experience for hundreds of millions of Instagram users, 
	we have very tight SLA on the metrics. 
	
	Instagram maintains a 5–9s reliability SLA, which means at any given time, the request failure rate should be less than 0.001%. 
	For performance, we actively monitor the throughput and latency of different Cassandra clusters, especially the P99 read latency. 
	
	After investigation, we found the JVM garbage collector (GC) contributed a lot to the latency spikes. 
	We defined a metric called GC stall percentage to measure the percentage of time a Cassandra server was doing stop-the-world GC (Young Gen GC) 
	and could not serve client requests.
	
	The graph shows that a Cassandra server instance could spend 2.5% of runtime on garbage collections instead of serving client requests. 
	The GC overhead obviously had a big impact on our P99 latency, so if we could lower the GC stall percentage, 
	we would be able to reduce our P99 latency significantly.
	
	Apache Cassandra is a distributed database with it’s own LSM tree-based storage engine written in Java. 
	We found that the components in the storage engine, like memtable, compaction, read/write path, etc., 
	created a lot of objects in the Java heap and generated a lot of overhead to JVM. 
	To reduce the GC impact from the storage engine, 
	we considered different approaches and ultimately decided to develop a C++ storage engine to replace existing ones.
	
	We did not want to build a new storage engine from scratch, so we decided to build the new storage engine on top of RocksDB. 
	
	RocksDB is an open source, high-performance embedded database for key-value data. 
	It’s written in C++, and provides official API language bindings for C++, C, and Java. 
	RocksDB is optimized for performance, especially on fast storage like SSD. 
	It’s widely used in the industry as the storage engine for MySQL, mongoDB, and other popular databases.

	The first challenge was that Cassandra does not have a pluggable storage engine architecture yet, 
	which means the existing storage engine is coupled together with other components in the database. 
	To find a balance between massive refactoring and quick iterations, we defined a new storage engine API, 
	including the most common read/write and streaming interfaces. 
	This way we could implement the new storage engine behind the API and inject it into the related code paths inside Cassandra.
	
	Secondly, Cassandra supports rich data types and table schema, while RocksDB provides purely key-value interfaces. 
	We carefully defined the encoding/decoding algorithms to support Cassandra’s data model within RocksDB’s data structure and 
	supported same-query semantics as original Cassandra.
	
	The third challenge was about streaming. Streaming is an important component for a distributed database like Cassandra. 
	Whenever we join or remove a node from a Cassandra cluster, Cassandra needs to stream data among different nodes to balance the load across the cluster. 
	The existing streaming implementation was based on the details in the current storage engine. 
	Accordingly, we had to decouple them from each other, make an abstraction layer, and re-implement the streaming using RocksDB APIs. 
	For high streaming throughput, we now stream data into temp sst files first, and then use the RocksDB ingest file API to bulk load them 
	into the RocksDB instance at once.
	
	After about a year of development and testing, we have finished a first version of the implementation and 
	successfully rolled it into several production Cassandra clusters in Instagram. 
	In one of our production clusters, the P99 read latency dropped from 60ms to 20ms. 
	We also observed that the GC stalls on that cluster dropped from 2.5% to 0.3%, which was a 10X reduction!
	
	We have open sourced our Rocksandra code base and benchmark framework, which you can download from Github to try out in your own environment! 
	https://github.com/Instagram/cassandra/tree/rocks_3.0
	https://github.com/Instagram/cassandra-aws-benchmark

Spotify
	
	Personalization at Spotify using Cassandra

	At Spotify we have have over 60 million active users who have access to a vast music catalog of over 30 million songs. 
	Our users have a choice to follow thousands of artists and hundreds of their friends and create their own music graph. 
	On our service they also discover new and existing content by experiencing a variety of music promotions 
	(album releases, artist promos), which get served over our ad platform. 
	These options have empowered our users and made them really engaged. 
	Over time they have created over 1.5 billion playlists and just last year they streamed over 7 billion hours worth of music.

	But at times an abundance of options has also made our users feel a bit lost. 
	How do you find that right playlist for your workout from over a billion playlists? 
	How do you discover new albums which are relevant to your taste? 
	We help our users discover and experience relevant content by personalizing their experience on our platform.

	Personalizing user experience involves learning their tastes and distastes in different contexts.
	A metal genre listener might not enjoy an announcement for a metal genre album when they are trying to put 
	their kid to sleep and playing kid’s music at night. 
	Serving them a recommendation for a kid’s music album might be more relevant in that context. 
	But this experience might not be relevant for another metal genre listener who doesn’t mind 
	receiving metal genre album recommendations during any context. 
	These two users with similar listening habits might have different preferences. 
	Personalizing their experiences on Spotify according to their respective taste in different contexts helps us make them more engaged.
	
	Overall Architecture
		In our personalization stack we are using Kafka for log collection, Storm for real-time event processing, 
		Crunch for running batch map-reduce jobs on Hadoop and Cassandra to store user profile attributes 
		and metadata about entities like playlists, artists, etc.

		In the diagram below logs are forwarded by Kafka producers, running on different services and emitting different kinds of events like completion of a song and delivery of an ad impression, to the Kafka broker. There are 2 sets of Kafka consumers, subscribed to different topics, for consuming events:

		Kafka Consumers for Hadoop which write these events to HDFS. All the raw logs on HDFS are then processed in Crunch to remove duplicate events, filter out unwanted fields and convert records into Avro Format.

		Kafka Consumer Spouts running within Storm topologies which stream the events for real-time computation.
		
		There are also other Crunch pipelines which ingest and generate metadata (genre, tempo, etc.) for different entities. These data records are stored in HDFS and also exported using  Crunch to Cassandra for real-time lookup in Storm pipelines. We will call the Cassandra cluster which stores entity metadata the Entity Metadata Store (EMS).

		The Storm pipelines process raw log events from Kafka, filter out unwanted events, decorate the entities with metadata fetched from EMS, group it per user and determine user level attributes by some algorithmic combination of aggregation and derivation. These user attributes when combined represent a user’s profile and they are stored in a Cassandra Cluster which we will call the User Profile Store (UPS). [spotify-cassandra-personalization.png] 



