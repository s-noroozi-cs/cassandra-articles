Architecture Requirements of Cassandra
	Cassandra was designed to address many architecture requirements. 
	The most important requirement is to ensure there is no single point of failure. 
	Its architecture is based on the understanding that system and hardware failures can and do occur.
	This means that if there are 100 nodes in a cluster and a node fails, the cluster should continue to operate.
	Cassandra addresses the problem of failures by employing a peer-to-peer distributed system 
	across homogeneous nodes where data is distributed among all nodes in the cluster.
	Each node frequently exchanges state information about itself and other nodes across the cluster using peer-to-peer gossip communication protocol. 

	This is in contrast to Hadoop where the namenode failure can cripple the entire system. 
	Another requirement is to have massive scalability so that a cluster can hold hundreds or thousands of nodes. 
	It should be possible to add a new node to the cluster without stopping the cluster.

	Further, the architecture should be highly distributed so that both processing and data can be distributed. 
	Also, high performance of read and write of data is expected so that the system can be used in real-time. 
	Let us explore the Cassandra architecture in the next section of the cassandra architecture tutorial.
	
	A sequentially written commit log on each node captures write activity to ensure data durability. 
	Data is then indexed and written to an in-memory structure, called a memtable, which resembles a write-back cache. 
	Each time the memory structure is full, the data is written to disk in an SSTables data file. 
	All writes are automatically partitioned and replicated throughout the cluster. 
	Cassandra periodically consolidates SSTables using a process called compaction, 
	discarding obsolete data marked for deletion with a tombstone. 
	To ensure all data across the cluster stays consistent, various repair mechanisms are employed.
	
	Client read or write requests can be sent to any node in the cluster. 
	When a client connects to a node with a request, that node serves as the coordinator for that particular client operation. 
	The coordinator acts as a proxy between the client application and the nodes that own the data being requested. 
	The coordinator determines which nodes in the ring should get the request based on how the cluster is configured.
	
	Commit log
		All data is written first to the commit log for durability. After all its data has been flushed to SSTables, it can be archived, deleted, or recycled. 
	
	SSTable
		A sorted string table (SSTable) is an immutable data file to which Cassandra writes memtables periodically. 
		SSTables are append only and stored on disk sequentially and maintained for each Cassandra table.


Key components for configuring cassandra
	
	Gossip
		A peer-to-peer communication protocol to discover and share location and state information about the other nodes in a Cassandra cluster. 
		Gossip information is also persisted locally by each node to use immediately when a node restarts.
		
	Partitioner
		A partitioner determines which node will receive the first replica of a piece of data, 
		and how to distribute other replicas across other nodes in the cluster. 
		Each row of data is uniquely identified by a primary key, which may be the same as its partition key, 
		but which may also include other clustering columns. 
		
		A partitioner is a hash function that derives a token from the primary key of a row. 
		The partitioner uses the token value to determine which nodes in the cluster receive the replicas of that row. 
		
		The Murmur3Partitioner is the default partitioning strategy for new Cassandra clusters and the right choice for new clusters in almost all cases.
		
		You must set the partitioner and assign the node a num_tokens value for each node. 
		The number of tokens you assign depends on the hardware capabilities of the system. 
		If not using virtual nodes (vnodes), use the initial_token setting instead.
	
	Replication factor
		The total number of replicas across the cluster. 
		A replication factor of 1 means that there is only one copy of each row on one node. 
		A replication factor of 2 means two copies of each row, where each copy is on a different node. 
		All replicas are equally important; there is no primary or master replica. 
		You define the replication factor for each datacenter. 
		Generally you should set the replication strategy greater than one, but no more than the number of nodes in the cluster.
	
	Replica placement strategy
		Cassandra stores copies (replicas) of data on multiple nodes to ensure reliability and fault tolerance. 
		A replication strategy determines which nodes to place replicas on. The first replica of data is simply the first copy; 
		it is not unique in any sense. The NetworkTopologyStrategy is highly recommended for most deployments 
		because it is much easier to expand to multiple datacenters when required by future expansion.

		When creating a keyspace, you must define the replica placement strategy and the number of replicas you want.
		
	Snitch
		A snitch defines groups of machines into datacenters and racks (the topology) that the replication strategy uses to place replicas.
		You must configure a snitch when you create a cluster. All snitches use a dynamic snitch layer, 
		which monitors performance and chooses the best replica for reading. 
		It is enabled by default and recommended for use in most deployments. 
		Configure dynamic snitch thresholds for each node in the cassandra.yaml configuration file.

		The default SimpleSnitch does not recognize datacenter or rack information. 
		Use it for single-datacenter deployments or single-zone in public clouds. 
		The GossipingPropertyFileSnitch is recommended for production. 
		It defines a node's datacenter and rack and uses gossip for propagating this information to other nodes.
	
Cassandra Architecture	
    - Cassandra is designed such that it has no master or slave nodes.
    - It has a ring-type architecture, that is, its nodes are logically distributed like a ring.
    - Data is automatically distributed across all the nodes.
    - Similar to HDFS, data is replicated across the nodes for redundancy.
    - Data is kept in memory and lazily written to the disk.
    - Hash values of the keys are used to distribute the data among nodes in the cluster.
	- Cassandra architecture supports multiple data centers.
	- Data can be replicated across data centers.
	
Effects of the Architecture
	Cassandra architecture enables transparent distribution of data to nodes. 
	This means you can determine the location of your data in the cluster based on the data. 
	Any node can accept any request as there are no masters or slaves. 
	If a node has the data, it will return the data. Else, it will send the request to the node that has the data.

	You can specify the number of replicas of the data to achieve the required level of redundancy. 
	For example, if the data is very critical, you may want to specify a replication factor of 4 or 5.
	If the data is not critical, you may specify just two. 
	
	It also provides tunable consistency, that is, the level of consistency can be specified as a trade-off with performance. 
	Transactions are always written to a commit log on disk so that they are durable. 
	
Cassandra Write Process
	The Cassandra write process ensures fast writes. Steps in the Cassandra write process are:
	
	1. Data is written to a commitlog on disk.
    2. The data is sent to a responsible node based on the hash value.
    3. Nodes write data to an in-memory table called memtable.
    4. From the memtable, data is written to an sstable in memory. 
	   Sstable stands for Sorted String table. This has a consolidated data of all the updates to the table.
    5. From the sstable, data is updated to the actual table.
    6. If the responsible node is down, data will be written to another node identified as tempnode. 
	The tempnode will hold the data temporarily till the responsible node comes alive.

	The diagram below [CassandraArchitecture_3.avif] depicts the write process when data is written to table A.
	
	Data is written to a commitlog on disk for persistence. It is also written to an in-memory memtable. 
	Memtable data is written to sstable which is used to update the actual table.
	
Rack
	The term ‘rack’ is usually used when explaining network topology. 
	A rack is a group of machines housed in the same physical box. 
	Each machine in the rack has its own CPU, memory, and hard disk. 
	However, the rack has no CPU, memory, or hard disk of its own.[CassandraArchitecture_4.avif]
	
	Features of racks are:
		- All machines in the rack are connected to the network switch of the rack
		- The rack’s network switch is connected to the cluster.
		- All machines on the rack have a common power supply. 
		  It is important to notice that a rack can fail due to two reasons: a network switch failure or a power supply failure.
		- If a rack fails, none of the machines on the rack can be accessed. So it would seem as though all the nodes on the rack are down.
	
Cassandra Read Process
	The Cassandra read process ensures fast reads. Read happens across all nodes in parallel. 
	If a node is down, data is read from the replica of the data. 
	Priority for the replica is assigned on the basis of distance. Features of the Cassandra read process are:

    - Data on the same node is given first preference and is considered data local.
    - Data on the same rack is given second preference and is considered rack local.
    - Data on the same data center is given third preference and is considered data center local.
    - Data in a different data center is given the least preference.

	Data in the memtable and sstable is checked first so that the data can be retrieved faster if it is already in memory.
	
Data Partitions
	Cassandra performs transparent distribution of data by horizontally partitioning the data in the following manner:
	- A hash value is calculated based on the primary key of the data.
    - The hash value of the key is mapped to a node in the cluster
    - The first copy of the data is stored on that node.
    - The distribution is transparent as you can both calculate the hash value and determine where a particular row will be stored.

	The following diagram depicts a four node cluster with token values of 0, 25, 50 and 75.[CassandraArchitecture_7.avif]
	
	For a given key, a hash value is generated in the range of 1 to 100. Keys with hash values in the range 
	1 to 25 are stored on the first node, 
	26 to 50 are stored on the second node, 
	51 to 75 are stored on the third node, and 
	76 to 100 are stored on the fourth node. 
	Please note that actual tokens and hash values in Cassandra are 127-bit positive integers.
	
Replication in Cassandra
	Replication refers to the number of replicas that are maintained for each row. 
	Replication provides redundancy of data for fault tolerance. 
	A replication factor of 3 means that 3 copies of data are maintained in the system.
	In this case, even if 2 machines are down, you can access your data from the third copy. 
	
	The default replication factor is 1. A replication factor of 1 means that a single copy of the data is maintained, 
	so if the node that has the data fails, you will lose the data.

	Cassandra allows replication based on nodes, racks, and data centers, 
	unlike HDFS that allows replication based on only nodes and racks. 
	Replication across data centers guarantees data availability even when a data center is down.
	
Network Topology
	Network topology refers to how the nodes, racks and data centers in a cluster are organized. 
	You can specify a network topology for your cluster as follows:
    - Specify in the Cassandra-topology.properties file.
    - Your data centers and racks can be specified for each node in the cluster.
    - Specify <ip-address>=<data center>:<rack name>.
    - For unknown nodes, a default can be specified.
    - You can also specify the hostname of the node instead of an IP address.

	An example of a topology configuration file.[CassandraArchitecture_8.avif]
	This file shows the topology defined for four nodes. The node with IP address 192.168.1.100 is mapped to data center DC1 and is present on the rack RAC1. 
	The node with IP address 192.168.2.200 is mapped to data center DC2 and is present on the rack RAC2.
	Similarly, the node with IP address 10.20.114.10 is mapped to data center DC2 and rack RAC1 and 
	the node with IP address 10.20.114.11 is mapped to data center DC2 and rack RAC1. 
	There is also a default assignment of data center DC1 and rack RAC1 so that any unassigned nodes will get this data center and rack.
	
Snitches
	Snitches define the topology in Cassandra. A snitch defines a group of nodes into racks and data centers. Two types of snitches are most popular:
	1. Simple Snitch - A simple snitch is used for single data centers with no racks.
    2. Property File Snitch - A property file snitch is used for multiple data centers with multiple racks.
	Replication in Cassandra is based on the snitches.
	
Gossip Protocol
	Cassandra uses a gossip protocol to communicate with nodes in a cluster. 
	It is an inter-node communication mechanism similar to the heartbeat protocol in Hadoop. 
	Cassandra uses the gossip protocol to discover the location of other nodes in the cluster and get state information of other nodes in the cluster.
	The gossip process runs periodically on each node and exchanges state information with three other nodes in the cluster. 
	Eventually, information is propagated to all cluster nodes. 
	Even if there are 1000 nodes, information is propagated to all the nodes within a few seconds.

	The following image depicts the gossip protocol process.[CassandraArchitecture_9.avif]
	In step 1, one node connects to three other nodes. 
	In step 2, each of the three nodes connects to three other nodes, thus connecting to nine nodes in total in step 2. 
	So a total of 13 nodes are connected in 2 steps.
	
Seed Nodes
	Seed nodes are used to bootstrap the gossip protocol. The features of seed nodes are:

    - They are specified in the configuration file Cassandra.yaml.
    - Seed nodes are used for bootstrapping the gossip protocol when a node is started or restarted.
    - They are used to achieve a steady state where each node is connected to every other node but are not required during the steady state.

	The diagram[CassandraArchitecture_10.avif] depicts a startup of a cluster with 2 seed nodes. 
	Initially, there is no connection between the nodes. On startup, two nodes connect to two other nodes that are specified as seed nodes. 
	Once all the four nodes are connected, seed node information is no longer required as steady state is achieved.
	
Virtual Nodes
	Virtual nodes in a Cassandra cluster are also called vnodes. 
	Vnodes can be defined for each physical node in the cluster. 
	Each node in the ring can hold multiple virtual nodes. By default, each node has 256 virtual nodes.
	
	Virtual nodes help achieve finer granularity in the partitioning of data, 
	and data gets partitioned into each virtual node using the hash value of the key. 
	On adding a new node to the cluster, the virtual nodes on it get equal portions of the existing data. 
	So there is no need to separately balance the data by running a balancer. 
	The image[CassandraArchitecture_11.avif] depicts a cluster with four physical nodes.
	Each physical node in the cluster has four virtual nodes. So there are 16 vnodes in the cluster.
	If 32TB of data is stored on the cluster, each vnode will get 2TB of data to store. 
	If another physical node with 4 virtual nodes is added to the cluster, 
	the data will be distributed to 20 vnodes in total such that each vnode will now have 1.6 TB of data.
	
Failure Scenarios

	Node Failure
		Cassandra is highly fault tolerant. The effects of node failure are as follows:
		- Other nodes detect the node failure.
		- Request for data on that node is routed to other nodes that have the replica of that data.
		- Writes are handled by a temporary node until the node is restarted.
		- Any memtable or sstable data that is lost is recovered from commitlog.
		- A node can be permanently removed using the nodetool utility
		The following image [CassandraArchitecture_13.avif] shows the concept of node failure.
		
	Disk Failure
		When a disk becomes corrupt, Cassandra detects the problem and takes corrective action. The effects of Disk Failure are as follows:
		- The data on the disk becomes inaccessible
		- Reading data from the node is not possible
		- This issue will be treated as node failure for that portion of data
		- Memtable and sstable will not be affected as they are in-memory tables
		- Commitlog has replicas and they will be used for recovery
		
	Rack Failure
		Sometimes, a rack could stop functioning due to power failure or a network switch problem. The effects of Rack Failure are as follows:
		- All the nodes on the rack become inaccessible
		- Reading data from the rack nodes is not possible
		- The reads will be routed to other replicas of the data
		- This will be treated as if each node in the rack has failed
		The following figure [CassandraArchitecture_14.avif] shows the concept of rack failure:
		
	Data Center Failure
		Data center failure occurs when a data center is shut down for maintenance or when it fails due to natural calamities. When that happens:
		- All data in the data center will become inaccessible.
		- All reads have to be routed to other data centers.
		- The replica copies in other data centers will be used.
		- Though the system will be operational, clients may notice slowdown due to network latency. 
		  This is because multiple data centers are normally located at physically different locations and connected by a wide area network.

How is data written?
	Cassandra processes data at several stages on the write path, 
	starting with the immediate logging of a write and ending in with a write of data to disk:
		- Logging data in the commit log
		- Writing data to the memtable
		- Flushing data from the memtable
		- Storing data on disk in SSTables
		
	Logging writes and memtable storage
		When a write occurs, Cassandra stores the data in a memory structure called memtable, 
		and to provide configurable durability, it also appends writes to the commit log on disk. 
		The commit log receives every write made to a Cassandra node, and these durable writes survive permanently 
		even if power fails on a node. The memtable is a write-back cache of data partitions that Cassandra looks up by key. 
		The memtable stores writes in sorted order until reaching a configurable limit, and then is flushed.
		
	Flushing data from the memtable
		To flush the data, Cassandra writes the data to disk, in the memtable-sorted order. 
		A partition index is also created on the disk that maps the tokens to a location on disk. 
		When the memtable content exceeds the configurable threshold or the commitlog space exceeds the commitlog_total_space_in_mb, 
		the memtable is put in a queue that is flushed to disk. 
		The queue can be configured with the memtable_heap_space_in_mb or memtable_offheap_space_in_mb setting in the cassandra.yaml file. 
		If the data to be flushed exceeds the memtable_cleanup_threshold, Cassandra blocks writes until the next flush succeeds. 
		You can manually flush a table using nodetool flushor nodetool drain (flushes memtables without listening for connections to other nodes). 
		To reduce the commit log replay time, the recommended best practice is to flush the memtable before you restart the nodes. 
		If a node stops working, replaying the commit log restores to the memtable the writes that were there before it stopped.
		
	

Data in the commit log is purged after its corresponding data in the memtable is flushed to an SSTable on disk.
