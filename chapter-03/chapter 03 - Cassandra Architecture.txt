Architecture Requirements of Cassandra
	Cassandra was designed to address many architecture requirements. 
	The most important requirement is to ensure there is no single point of failure. 
	Its architecture is based on the understanding that system and hardware failures can and do occur.
	This means that if there are 100 nodes in a cluster and a node fails, the cluster should continue to operate.
	Cassandra addresses the problem of failures by employing a peer-to-peer distributed system 
	across homogeneous nodes where data is distributed among all nodes in the cluster.
	Each node frequently exchanges state information about itself and other nodes across the cluster using peer-to-peer gossip communication protocol. 

	This is in contrast to Hadoop where the namenode failure can cripple the entire system. 
	Another requirement is to have massive scalability so that a cluster can hold hundreds or thousands of nodes. 
	It should be possible to add a new node to the cluster without stopping the cluster.

	Further, the architecture should be highly distributed so that both processing and data can be distributed. 
	Also, high performance of read and write of data is expected so that the system can be used in real-time. 
	Let us explore the Cassandra architecture in the next section of the cassandra architecture tutorial.
	
	A sequentially written commit log on each node captures write activity to ensure data durability. 
	Data is then indexed and written to an in-memory structure, called a memtable, which resembles a write-back cache. 
	Each time the memory structure is full, the data is written to disk in an SSTables data file. 
	All writes are automatically partitioned and replicated throughout the cluster. 
	Cassandra periodically consolidates SSTables using a process called compaction, 
	discarding obsolete data marked for deletion with a tombstone. 
	To ensure all data across the cluster stays consistent, various repair mechanisms are employed.
	
	Client read or write requests can be sent to any node in the cluster. 
	When a client connects to a node with a request, that node serves as the coordinator for that particular client operation. 
	The coordinator acts as a proxy between the client application and the nodes that own the data being requested. 
	The coordinator determines which nodes in the ring should get the request based on how the cluster is configured.
	
	Commit log
		All data is written first to the commit log for durability. After all its data has been flushed to SSTables, it can be archived, deleted, or recycled. 
	
	SSTable
		A sorted string table (SSTable) is an immutable data file to which Cassandra writes memtables periodically. 
		SSTables are append only and stored on disk sequentially and maintained for each Cassandra table.


Key components for configuring cassandra
	
	Gossip
		A peer-to-peer communication protocol to discover and share location and state information about the other nodes in a Cassandra cluster. 
		Gossip information is also persisted locally by each node to use immediately when a node restarts.
		
	Partitioner
		A partitioner determines which node will receive the first replica of a piece of data, 
		and how to distribute other replicas across other nodes in the cluster. 
		Each row of data is uniquely identified by a primary key, which may be the same as its partition key, 
		but which may also include other clustering columns. 
		
		A partitioner is a hash function that derives a token from the primary key of a row. 
		The partitioner uses the token value to determine which nodes in the cluster receive the replicas of that row. 
		
		The Murmur3Partitioner is the default partitioning strategy for new Cassandra clusters and the right choice for new clusters in almost all cases.
		
		You must set the partitioner and assign the node a num_tokens value for each node. 
		The number of tokens you assign depends on the hardware capabilities of the system. 
		If not using virtual nodes (vnodes), use the initial_token setting instead.
	
	Replication factor
		The total number of replicas across the cluster. 
		A replication factor of 1 means that there is only one copy of each row on one node. 
		A replication factor of 2 means two copies of each row, where each copy is on a different node. 
		All replicas are equally important; there is no primary or master replica. 
		You define the replication factor for each datacenter. 
		Generally you should set the replication strategy greater than one, but no more than the number of nodes in the cluster.
	
	Replica placement strategy
		Cassandra stores copies (replicas) of data on multiple nodes to ensure reliability and fault tolerance. 
		A replication strategy determines which nodes to place replicas on. The first replica of data is simply the first copy; 
		it is not unique in any sense. The NetworkTopologyStrategy is highly recommended for most deployments 
		because it is much easier to expand to multiple datacenters when required by future expansion.

		When creating a keyspace, you must define the replica placement strategy and the number of replicas you want.
		
	Snitch
		A snitch defines groups of machines into datacenters and racks (the topology) that the replication strategy uses to place replicas.
		You must configure a snitch when you create a cluster. All snitches use a dynamic snitch layer, 
		which monitors performance and chooses the best replica for reading. 
		It is enabled by default and recommended for use in most deployments. 
		Configure dynamic snitch thresholds for each node in the cassandra.yaml configuration file.

		The default SimpleSnitch does not recognize datacenter or rack information. 
		Use it for single-datacenter deployments or single-zone in public clouds. 
		The GossipingPropertyFileSnitch is recommended for production. 
		It defines a node's datacenter and rack and uses gossip for propagating this information to other nodes.
	
Cassandra Architecture	
    - Cassandra is designed such that it has no master or slave nodes.
    - It has a ring-type architecture, that is, its nodes are logically distributed like a ring.
    - Data is automatically distributed across all the nodes.
    - Similar to HDFS, data is replicated across the nodes for redundancy.
    - Data is kept in memory and lazily written to the disk.
    - Hash values of the keys are used to distribute the data among nodes in the cluster.
	- Cassandra architecture supports multiple data centers.
	- Data can be replicated across data centers.
	
Effects of the Architecture
	Cassandra architecture enables transparent distribution of data to nodes. 
	This means you can determine the location of your data in the cluster based on the data. 
	Any node can accept any request as there are no masters or slaves. 
	If a node has the data, it will return the data. Else, it will send the request to the node that has the data.

	You can specify the number of replicas of the data to achieve the required level of redundancy. 
	For example, if the data is very critical, you may want to specify a replication factor of 4 or 5.
	If the data is not critical, you may specify just two. 
	
	It also provides tunable consistency, that is, the level of consistency can be specified as a trade-off with performance. 
	Transactions are always written to a commit log on disk so that they are durable. 
	
Cassandra Write Process
	The Cassandra write process ensures fast writes. Steps in the Cassandra write process are:
	
	1. Data is written to a commitlog on disk.
    2. The data is sent to a responsible node based on the hash value.
    3. Nodes write data to an in-memory table called memtable.
    4. From the memtable, data is written to an sstable in memory. 
	   Sstable stands for Sorted String table. This has a consolidated data of all the updates to the table.
    5. From the sstable, data is updated to the actual table.
    6. If the responsible node is down, data will be written to another node identified as tempnode. 
	The tempnode will hold the data temporarily till the responsible node comes alive.

	The diagram below [CassandraArchitecture_3.avif] depicts the write process when data is written to table A.
	
	Data is written to a commitlog on disk for persistence. It is also written to an in-memory memtable. 
	Memtable data is written to sstable which is used to update the actual table.
	
Rack
	The term ‘rack’ is usually used when explaining network topology. 
	A rack is a group of machines housed in the same physical box. 
	Each machine in the rack has its own CPU, memory, and hard disk. 
	However, the rack has no CPU, memory, or hard disk of its own.[CassandraArchitecture_4.avif]
	
	Features of racks are:
		- All machines in the rack are connected to the network switch of the rack
		- The rack’s network switch is connected to the cluster.
		- All machines on the rack have a common power supply. 
		  It is important to notice that a rack can fail due to two reasons: a network switch failure or a power supply failure.
		- If a rack fails, none of the machines on the rack can be accessed. So it would seem as though all the nodes on the rack are down.
	
Cassandra Read Process
	The Cassandra read process ensures fast reads. Read happens across all nodes in parallel. 
	If a node is down, data is read from the replica of the data. 
	Priority for the replica is assigned on the basis of distance. Features of the Cassandra read process are:

    - Data on the same node is given first preference and is considered data local.
    - Data on the same rack is given second preference and is considered rack local.
    - Data on the same data center is given third preference and is considered data center local.
    - Data in a different data center is given the least preference.

	Data in the memtable and sstable is checked first so that the data can be retrieved faster if it is already in memory.
	
Data Partitions
	Cassandra performs transparent distribution of data by horizontally partitioning the data in the following manner:
	- A hash value is calculated based on the primary key of the data.
    - The hash value of the key is mapped to a node in the cluster
    - The first copy of the data is stored on that node.
    - The distribution is transparent as you can both calculate the hash value and determine where a particular row will be stored.

	The following diagram depicts a four node cluster with token values of 0, 25, 50 and 75.[CassandraArchitecture_7.avif]
	
	For a given key, a hash value is generated in the range of 1 to 100. Keys with hash values in the range 
	1 to 25 are stored on the first node, 
	26 to 50 are stored on the second node, 
	51 to 75 are stored on the third node, and 
	76 to 100 are stored on the fourth node. 
	Please note that actual tokens and hash values in Cassandra are 127-bit positive integers.
	
Replication in Cassandra
	Replication refers to the number of replicas that are maintained for each row. 
	Replication provides redundancy of data for fault tolerance. 
	A replication factor of 3 means that 3 copies of data are maintained in the system.
	In this case, even if 2 machines are down, you can access your data from the third copy. 
	
	The default replication factor is 1. A replication factor of 1 means that a single copy of the data is maintained, 
	so if the node that has the data fails, you will lose the data.

	Cassandra allows replication based on nodes, racks, and data centers, 
	unlike HDFS that allows replication based on only nodes and racks. 
	Replication across data centers guarantees data availability even when a data center is down.
	
Network Topology
	Network topology refers to how the nodes, racks and data centers in a cluster are organized. 
	You can specify a network topology for your cluster as follows:
    - Specify in the Cassandra-topology.properties file.
    - Your data centers and racks can be specified for each node in the cluster.
    - Specify <ip-address>=<data center>:<rack name>.
    - For unknown nodes, a default can be specified.
    - You can also specify the hostname of the node instead of an IP address.

	An example of a topology configuration file.[CassandraArchitecture_8.avif]
	This file shows the topology defined for four nodes. The node with IP address 192.168.1.100 is mapped to data center DC1 and is present on the rack RAC1. 
	The node with IP address 192.168.2.200 is mapped to data center DC2 and is present on the rack RAC2.
	Similarly, the node with IP address 10.20.114.10 is mapped to data center DC2 and rack RAC1 and 
	the node with IP address 10.20.114.11 is mapped to data center DC2 and rack RAC1. 
	There is also a default assignment of data center DC1 and rack RAC1 so that any unassigned nodes will get this data center and rack.
	
Snitches
	Snitches define the topology in Cassandra. A snitch defines a group of nodes into racks and data centers. Two types of snitches are most popular:
	1. Simple Snitch - A simple snitch is used for single data centers with no racks.
    2. Property File Snitch - A property file snitch is used for multiple data centers with multiple racks.
	Replication in Cassandra is based on the snitches.
	
Gossip Protocol
	Cassandra uses a gossip protocol to communicate with nodes in a cluster. 
	It is an inter-node communication mechanism similar to the heartbeat protocol in Hadoop. 
	Cassandra uses the gossip protocol to discover the location of other nodes in the cluster and get state information of other nodes in the cluster.
	The gossip process runs periodically on each node and exchanges state information with three other nodes in the cluster. 
	Eventually, information is propagated to all cluster nodes. 
	Even if there are 1000 nodes, information is propagated to all the nodes within a few seconds.

	The following image depicts the gossip protocol process.[CassandraArchitecture_9.avif]
	In step 1, one node connects to three other nodes. 
	In step 2, each of the three nodes connects to three other nodes, thus connecting to nine nodes in total in step 2. 
	So a total of 13 nodes are connected in 2 steps.
	
Seed Nodes
	Seed nodes are used to bootstrap the gossip protocol. The features of seed nodes are:

    - They are specified in the configuration file Cassandra.yaml.
    - Seed nodes are used for bootstrapping the gossip protocol when a node is started or restarted.
    - They are used to achieve a steady state where each node is connected to every other node but are not required during the steady state.

	The diagram[CassandraArchitecture_10.avif] depicts a startup of a cluster with 2 seed nodes. 
	Initially, there is no connection between the nodes. On startup, two nodes connect to two other nodes that are specified as seed nodes. 
	Once all the four nodes are connected, seed node information is no longer required as steady state is achieved.
	
Virtual Nodes
	Virtual nodes in a Cassandra cluster are also called vnodes. 
	Vnodes can be defined for each physical node in the cluster. 
	Each node in the ring can hold multiple virtual nodes. By default, each node has 256 virtual nodes.
	
	Virtual nodes help achieve finer granularity in the partitioning of data, 
	and data gets partitioned into each virtual node using the hash value of the key. 
	On adding a new node to the cluster, the virtual nodes on it get equal portions of the existing data. 
	So there is no need to separately balance the data by running a balancer. 
	The image[CassandraArchitecture_11.avif] depicts a cluster with four physical nodes.
	Each physical node in the cluster has four virtual nodes. So there are 16 vnodes in the cluster.
	If 32TB of data is stored on the cluster, each vnode will get 2TB of data to store. 
	If another physical node with 4 virtual nodes is added to the cluster, 
	the data will be distributed to 20 vnodes in total such that each vnode will now have 1.6 TB of data.
	
Failure Scenarios

	Node Failure
		Cassandra is highly fault tolerant. The effects of node failure are as follows:
		- Other nodes detect the node failure.
		- Request for data on that node is routed to other nodes that have the replica of that data.
		- Writes are handled by a temporary node until the node is restarted.
		- Any memtable or sstable data that is lost is recovered from commitlog.
		- A node can be permanently removed using the nodetool utility
		The following image [CassandraArchitecture_13.avif] shows the concept of node failure.
		
	Disk Failure
		When a disk becomes corrupt, Cassandra detects the problem and takes corrective action. The effects of Disk Failure are as follows:
		- The data on the disk becomes inaccessible
		- Reading data from the node is not possible
		- This issue will be treated as node failure for that portion of data
		- Memtable and sstable will not be affected as they are in-memory tables
		- Commitlog has replicas and they will be used for recovery
		
	Rack Failure
		Sometimes, a rack could stop functioning due to power failure or a network switch problem. The effects of Rack Failure are as follows:
		- All the nodes on the rack become inaccessible
		- Reading data from the rack nodes is not possible
		- The reads will be routed to other replicas of the data
		- This will be treated as if each node in the rack has failed
		The following figure [CassandraArchitecture_14.avif] shows the concept of rack failure:
		
	Data Center Failure
		Data center failure occurs when a data center is shut down for maintenance or when it fails due to natural calamities. When that happens:
		- All data in the data center will become inaccessible.
		- All reads have to be routed to other data centers.
		- The replica copies in other data centers will be used.
		- Though the system will be operational, clients may notice slowdown due to network latency. 
		  This is because multiple data centers are normally located at physically different locations and connected by a wide area network.

How is data written?
	Cassandra processes data at several stages on the write path, 
	starting with the immediate logging of a write and ending in with a write of data to disk:
		- Logging data in the commit log
		- Writing data to the memtable
		- Flushing data from the memtable
		- Storing data on disk in SSTables
		
	Logging writes and memtable storage
		When a write occurs, Cassandra stores the data in a memory structure called memtable, 
		and to provide configurable durability, it also appends writes to the commit log on disk. 
		The commit log receives every write made to a Cassandra node, and these durable writes survive permanently 
		even if power fails on a node. The memtable is a write-back cache of data partitions that Cassandra looks up by key. 
		The memtable stores writes in sorted order until reaching a configurable limit, and then is flushed.
		
	Flushing data from the memtable
		To flush the data, Cassandra writes the data to disk, in the memtable-sorted order. 
		A partition index is also created on the disk that maps the tokens to a location on disk. 
		When the memtable content exceeds the configurable threshold or the commitlog space exceeds the commitlog_total_space_in_mb, 
		the memtable is put in a queue that is flushed to disk. 
		The queue can be configured with the memtable_heap_space_in_mb or memtable_offheap_space_in_mb setting in the cassandra.yaml file. 
		If the data to be flushed exceeds the memtable_cleanup_threshold, Cassandra blocks writes until the next flush succeeds. 
		You can manually flush a table using nodetool flushor nodetool drain (flushes memtables without listening for connections to other nodes). 
		To reduce the commit log replay time, the recommended best practice is to flush the memtable before you restart the nodes. 
		If a node stops working, replaying the commit log restores to the memtable the writes that were there before it stopped.
		Data in the commit log is purged after its corresponding data in the memtable is flushed to an SSTable on disk.
		
	Storing data on disk in SSTables
		Memtables and SSTables are maintained per table. The commit log is shared among tables. 
		SSTables are immutable, not written to again after the memtable is flushed. 
		Consequently, a partition is typically stored across multiple SSTable files. 
		A number of other SSTable structures exist to assist read operations: [dml_write-process_12.png]
		
		For each SSTable, Cassandra creates these structures:
			Data (Data.db)
				The SSTable data

			Primary Index (Index.db)
				Index of the row keys with pointers to their positions in the data file

			Bloom filter (Filter.db)
				A structure stored in memory that checks if row data exists in the memtable before accessing SSTables on disk
			
			Compression Information (CompressionInfo.db)
				A file holding information about uncompressed data length, chunk offsets and other compression information

			Statistics (Statistics.db)
				Statistical metadata about the content of the SSTable

			Digest (Digest.crc32, Digest.adler32, Digest.sha1)
				A file holding adler32 checksum of the data file

			CRC (CRC.db)
				A file holding the CRC32 for chunks in an uncompressed file.

			SSTable Index Summary (SUMMARY.db)
				A sample of the partition index stored in memory

			SSTable Table of Contents (TOC.txt)
				A file that stores the list of all components for the SSTable TOC

			Secondary Index (SI_.*.db)
				Built-in secondary index. Multiple SIs may exist per SSTable


		The SSTables are files stored on disk. The naming convention for SSTable files has changed with Cassandra 2.2 and later to shorten the file path. 
		The data files are stored in a data directory that varies with installation. 
		For each keyspace, a directory within the data directory stores each table. 
		For example following path represents a data file. 
			/data/data/ks1/cf1-5be396077b811e3a3ab9dc4b9ac088d/la-1-big-Data.db 
		
		ks1 represents the keyspace name to distinguish the keyspace for streaming or bulk loading data. 
		A hexadecimal string, 5be396077b811e3a3ab9dc4b9ac088d in this example, is appended to table names to represent unique table IDs.
		Cassandra creates a subdirectory for each table, which allows you to symlink a table to a chosen physical drive or data volume. 
		This provides the capability to move very active tables to faster media, such as SSDs for better performance, 
		and also divides tables across all attached storage devices for better I/O balance at the storage layer.
		
Replication
	The data in each keyspace is replicated with a replication factor. 
	The most common replication factor used is three. 
	There is one primary replica of data that resides with the token owner node as explained in the data partitioning section. 
	The remainder of replicas is placed by Cassandra on specific nodes using the replica placement strategy. 
	All replicas are equally important for all database operations except for a few cluster mutation operations.

	There are two settings that mainly impact replica placement. 
		First is snitch, which determines the data center, and the rack a Cassandra node belongs to, and it is set at the node level. 
			They inform Cassandra about the network topology so that requests are routed efficiently and 
			allow Cassandra to distribute replicas by grouping machines into data centers and racks. 
			GossipingPropertyFileSnitch is the goto snitch for any cluster deployment. 
			It uses a configuration file called Cassandra-rackdc.properties on each node. 
			It contains the rack and data center name which hosts the node. 
			There is cloud-specific snitch available for AWS and GCP. 

		The second setting is the replication strategy. 
			The replication strategy is set at the keyspace level. 
			There are two strategies: 
				SimpleStrategy and NetworkTopologyStrategy. 
					The SimpleStrategy does not consider racks and multiple data centers. It places data replicas on nodes sequentially. 
					The NetworkTopologyStrategy is rack aware and data center aware. 
					SimpleStrategy should be only used for temporary and small cluster deployments, 
					for all other clusters NetworkTopologyStrategy is highly recommended. 
					A keyspace definition when used with NetworkTopologyStrategy specifies the number of replicas per data center as:
						cqlsh> create keyspace ks with replication = {'class' : 'NetworkTopologyStrategy', dc_1: 3, dc_2: 1}

Gossip protocol - failure detection
	When building a system on top of a set of wildly uncooperative and unruly computers you have knowledge problems: 
		knowing when other nodes are dead; 
		knowing when nodes become alive; 
		getting information about other nodes so you can make local decisions, 
		like knowing which node should handle a request based on a scheme for assigning nodes to a certain range of users; 
		learning about new configuration data; agreeing on data values; and so on.

	How do you solve these problems?
		A common centralized approach is to use a database and all nodes query it for information. 
		Obvious availability and performance issues for large distributed clusters. 
		Another approach is to use Paxos, a protocol for solving consensus in a network 
		to maintain strict consistency requirements for small groups of unreliable processes. 
		Not practical when larger number of nodes are involved.

	what's the super cool decentralized way to bring order to large clusters?
		Gossip protocols, which maintain relaxed consistency requirements amongst a very large group of nodes. 
		A gossip protocol is simple in concept. Each nodes sends out some data to a set of other nodes. 
		Data propagates through the system node by node like a virus. 
		Eventually data propagates to every node in the system. It's a way for nodes to build a global map from limited local interactions.

	A node only has to send to a subset of other nodes. That's it.

	In a distributed system you need at least two independent sources of information to mark a node down. 
	It's not enough to simply say because your node can't contact another node that the other node is down. 
	It's quite possible that your node is broken and the other node is fine. 
	But if other nodes in the system also see that other node is dead then you can with some confidence conclude that that node is dead. 
	Many complex hard to debug bugs are hidden here.

	Another advantage of this approach is data could flow directly into your monitoring system 
	rather than having a completely separate monitoring subsystem bolted on.

	There are 3 main types of gossip protocol:
		1. Dissemination Protocols :
				These protocols are also referred to as rumor-mongering protocols because they use gossip to spread information throughout the network, 
				they flood the members of the network with gossips in a way that produces the worst-case load.
		2. Anti-Entropy Protocols :
			These are used to repair the replicated data by comparing them and modifying the comparisons.
		3. Protocols that compute aggregates :
			These protocols work by or compute an aggregate of the network by sampling information at the nodes and 
			they combine the values to acquire a system-wide value – the largest value for some measurement nodes are making, smallest, etc.

Gossip protocol
	The Gossip protocol is used to repair the problems caused by multicasting; 
	it is a type of communication where a piece of information or gossip in this scenario, 
	is sent from one or more nodes to a set of other nodes in a network. 
	This is useful when a group of clients in the network require the same data at the same time. 
	But there are many problems that occur during multicasting, 
	if there are many nodes present at the recipient end, latency increases; the average time for a receiver to receive a multicast.
	
	To get this multicast message or gossip across the desired targets in the group, 
	the gossip protocol sends out the gossip periodically to random nodes in the network, 
	once a random node receives the gossip, it is said to be infected due to the gossip. 
	Now the random node that receives the gossip does the same thing as the sender, 
	it sends multiple copies of the gossip to random targets. This process continues until the target nodes get the multicast. 
	This process turns the infected nodes to uninfected nodes after sending the gossip out to random nodes.

	Gossip Dissemination
		Use random selection of nodes to pass on information to ensure it reaches all the nodes in the cluster without flooding the network.

	Problem
		In a cluster of nodes, each node needs to pass metadata information it has, 
		to all the other nodes in the cluster, without depending on a shared storage. 
		In a large cluster, if all servers communicate with all the other servers, 
		a lot of network bandwidth can be consumed. 
		Information should reach all the nodes even when some network links are experiencing issues.

	Solution
		Cluster nodes use gossip style communication to propagate state updates. 
		Each node selects a random node to pass the information it has. 
		This is done at a regular interval, say every 1 second. 
		Each time, a random node is selected to pass on the information.

	The Gossip protocol is the internal communication technique for nodes in a cluster to talk to each other. 
	Gossip is an efficient, lightweight, reliable inter-nodal broadcast protocol for diffusing data. 
	It's decentralized, "epidemic", fault tolerant and a peer-to-peer communication protocol. 
	Cassandra uses gossiping for peer discovery and metadata propagation.[gossip-ring]

	The gossip process runs every second for every node and exchange state messages with up to three other nodes in the cluster. 
	Since the whole process is decentralized, there is nothing or no one that coordinates each node to gossip. 
	Each node independently will always select one to three peers to gossip with. 
	It will always select a live peer (if any) in the cluster, it will probabilistically pick a seed node 
	from the cluster or maybe it will probabilistically select an unavailable node.[gossip-msg-exchange]

	The Gossip messaging is very similar to the TCP three-way handshake. 
	With a regular broadcast protocol, there could only have been one message per round, 
	and the data can be allowed to gradually spread through the cluster. 
	But with the gossip protocol, having three messages for each round of gossip adds a degree of anti-entropy. 
	This process allows obtaining "convergence" of data shared between the two interacting nodes much faster.

		SYN: The node initiating the round of gossip sends the SYN message which contains a compendium of the nodes in the cluster. 
		It contains tuples of the IP address of a node in the cluster, the generation and the heartbeat version of the node.

		ACK: The peer after receiving SYN message compares its own metadata information with the one sent by the initiator 
		and produces a diff. ACK contains two kinds of data. 
		One part consists of updated metadata information (AppStates) that the peer has but the initiator doesn't, 
		and the other part consists of digest of nodes the initiator has that the peer doesn't.

		ACK2: The initiator receives the ACK from peer and updates its metadata from the AppStates and 
		sends back ACK2 containing the metadata information the peer has requested for. 
		The peer receives ACK2, updates its metadata and the round of gossip concludes.

	An important note here is that this messaging protocol will cause only a constant amount of network traffic. 
	Since the broadcasting of the initial digest is limited to three nodes 
	and data convergence occurs through a pretty constant ACK and ACK2, 
	there will not be much of network spike. Although, if a node gets UP, all the nodes might want 
	to send data to that peer, causing the Gossip Storm.

	So how does a new node get the idea of whom to start gossiping with? 
		Well, Cassandra has many seed provider implementations that provide a list of seed addresses to the new node 
		and starts gossiping with one of them right away. After its first round of gossip, 
		it will now possess cluster membership information about all the other nodes in the cluster 
		and can then gossip with the rest of them.

	Well, how do we get to know if a node is UP/DOWN? 
		The Failure Detector is the only component inside Cassandra(only the primary gossip class can mark a node UP besides) to do so. 
		It is a heartbeat listener and marks down the timestamps and keeps backlogs of intervals 
		at which it receives heartbeat updates from each peer. 
		Based on the reported data, it determines whether a peer is UP/DOWN.

	How does a node being UP/DOWN affect the cluster? 
		The write operations stay unaffected. If a node does not get an acknowledgment for a write to a peer, 
		it simply stores it up as a hint. The nodes will stop sending read requests to a peer in DOWN state 
		and probabilistically gossiping can be tried upon since its an unavailable node, 
		as we have already discussed early on. 
		All repair, stream sessions are closed as well when an unavailable node is involved.

	What if a peer is responding very slowly or timing out? 
		Cassandra has another component called the Dynamic Snitch, which records and analyses 
		latencies of read requests to peer nodes. It ranks latencies of peers in a rolling window and 
		recalculates it every 100ms and resets the scores every 10mins to allow 
		for any other events(Eg: Garbage Collection) delaying the response time of a peer. 
		In this way, the Dynamic Snitch helps you identify the slow nodes and avoid them when indulging in Gossip.

Snitches
	In Cassandra Snitch is very useful and snitch is also helps in keep record to avoid storing multiple replicas of data on the same rack. 
	In Cassandra, it is very important aspects to avoid multiple replica. 
	In replication strategy we assign number of replica and also we define the data-center. 
	This information is very helpful for snitch to identify the node and which rack belong to. 

	In Cassandra, snitch job is to determine which data centers and racks it should use to read data from and write data to. 
	In Cassandra, all snitch are dynamic by default. 

	They inform Cassandra about the network topology so that requests are routed efficiently and 
	allows Cassandra to distribute replicas by grouping machines into datacenters and racks. 
	Specifically, the replication strategy places the replicas based on the information provided by the new snitch. 
	All nodes must return to the same rack and datacenter. 
	Cassandra does its best not to have more than one replica on the same rack (which is not necessarily a physical location).

	In cassandra, the snitch has two functions:
		1. it teaches Cassandra enough about your network topology to route requests efficiently.
		2. it allows Cassandra to spread replicas around your cluster to avoid correlated failures. 
		It does this by grouping machines into "datacenters" and "racks."

	Dynamic snitching
		The dynamic snitch monitor read latencies to avoid reading from hosts that have slowed down.
		Monitors the performance of reads from the various replicas and chooses the best replica based on this history.

	Snitch classes
		The endpoint_snitch parameter in cassandra.yaml should be set to the class that implements IEndPointSnitch 
		which will be wrapped by the dynamic snitch and decide if two endpoints are in the same data center or on the same rack. 
		Out of the box, Cassandra provides the snitch implementations:

		GossipingPropertyFileSnitch
			This should be your go-to snitch for production use. 
			The rack and datacenter for the local node are defined in cassandra-rackdc.properties 
			and propagated to other nodes via gossip. If cassandra-topology.properties exists, 
			it is used as a fallback, allowing migration from the PropertyFileSnitch.
			Automatically updates all nodes using gossip when adding new nodes and is recommended for production.
		
		SimpleSnitch
			Treats Strategy order as proximity. This can improve cache locality when disabling read repair. 
			Only appropriate for single-datacenter deployments.

		PropertyFileSnitch
			Proximity is determined by rack and data center, 
			which are explicitly configured in cassandra-topology.properties.

		Ec2Snitch
			Appropriate for EC2 deployments in a single Region, or in multiple regions 
			with inter-region VPC enabled (available since the end of 2017, see AWS announcement). 
			Loads Region and Availability Zone information from the EC2 API. 
			The Region is treated as the datacenter, and the Availability Zone as the rack. 
			Only private IPs are used, so this will work across multiple regions only if inter-region VPC is enabled.

		Ec2MultiRegionSnitch
			Uses public IPs as broadcast_address to allow cross-region connectivity (thus, 
			you should set seed addresses to the public IP as well). You will need to open the 
			storage_port or ssl_storage_port on the public IP firewall (For intra-Region traffic, 
			Cassandra will switch to the private IP after establishing a connection).

		RackInferringSnitch
			Proximity is determined by rack and data center, which are assumed to correspond 
			to the 3rd and 2nd octet of each node’s IP address, respectively. 
			Unless this happens to match your deployment conventions, 
			this is best used as an example of writing a custom Snitch class and is provided in that spirit.

		GoogleCloudSnitch
			Use the GoogleCloudSnitch for Cassandra deployments on Google Cloud Platform across one or more regions.

		CloudstackSnitch
			Use the CloudstackSnitch for Apache Cloudstack environments.

Data Distribution
	In Cassandra data distribution and replication go together. 
	In Cassandra distribution and replication depending on the three thing such that partition key, key value and Token range.

	Distribution provides power and resiliencwe
		One important Cassandra attribute is that its databases are distributed. 
		That yields both technical and business advantages. 
		Cassandra databases easily scale when an application is under high stress, 
		and the distribution also prevents data loss from any given datacenter’s hardware failure. 
		A distributed architecture also brings technical power; 
		for example, a developer can tweak the throughput of read queries or write queries in isolation.

		"Distributed" means that Cassandra can run on multiple machines while appearing to users as a unified whole. 
		There is little point in running Cassandsra as a single node, 
		although it is very helpful to do so to help you get up to speed on how it works. 
		But to get the maximum benefit out of Cassandra, you would run it on multiple machines.

		Since it is a distributed database, Cassandra can (and usually does) have multiple nodes. 
		A node represents a single instance of Cassandra. 
		These nodes communicate with one another through a protocol called gossip, 
		which is a process of computer peer-to-peer communication. 
		Cassandra also has a masterless architecture 
		– any node in the database can provide the exact same functionality as any other node – 
		contributing to Cassandra’s robustness and resilience. 
		Multiple nodes can be organized logically into a cluster, or "ring". You can also have multiple datacenters.

Staged event-driven architecture
	The staged event-driven architecture (SEDA) refers to an approach to software architecture that decomposes a complex, 
	event-driven application into a set of stages connected by queues. 
	It avoids the high overhead associated with thread-based concurrency models 
	(i.e. locking, unlocking, and polling for locks), and decouples event and thread scheduling from application logic. 
	By performing admission control on each event queue, the service can be well-conditioned to load, 
	preventing resources from being overcommitted when demand exceeds service capacity.
	
	SEDA employs dynamic control to automatically tune runtime parameters (such as the scheduling parameters of each stage) 
	as well as to manage load (like performing adaptive load shedding). 
	Decomposing services into a set of stages also enables modularity and code reuse, 
	as well as the development of debugging tools for complex event-driven applications.

	Let us consider a sample application which listens to a stream of messages in a Message Queue, 
	reads them and processes them based on some business logic, and sends Email. 
	A typical implementation of the above application uses multiple application-level threads to process messages. 
	A single thread will read a single message from the Message Queue and is responsible for processing a single message. 
	The horizontal scaling will be achieved by managing multiple processes deployed in multiple hosts.

	One of the problems with the logic for processing a message is organized as a single continuous flow of code 
	which makes the threads wait for any expensive IO operations involved in the business logic. 
	The impact of the expensive IO operations on the overall throughput of the system is barely noticeable for low scale systems. 
	But for the systems which need to process a million messages per second, 
	these expensive operations will introduce bottleneck for the overall throughput of the system.

	SEDA aims to resolve the above-mentioned problems by decomposing the application code into multiple stages, 
	and each stage communicates with each other using the queues.

Write Path Anatomy
	Coordinator
		When a request is sent to any Cassandra node, this node acts as a proxy for the application (actually, the Cassandra driver) 
		and the nodes involved in the request flow. This proxy node is called as the coordinator. 
		The coordinator is responsible for managing the entire request path and to respond back to the client.
		Besides, sometimes when the coordinator forwards a write request to the replica nodes, 
		they may happen to be unavailable at this very moment. In this case, the coordinator plays an important role 
		implementing a mechanism called Hinted Handoff.

	Partitioner
		Basically, for each node in the Cassandra cluster (Cassandra ring) is assigned a range of tokens.
		Cassandra distributes data across the cluster using a Consistent Hashing algorithm and, starting from version 1.2, 
		it also implements the concept of virtual nodes (vnodes), where each node owns a large number of small token ranges 
		in order to improve token reorganization and avoid hotspots in the cluster, that is, some nodes storing much more data than the others. Virtual nodes also allow to add and remove nodes in the cluster more easily and manages the token assignment automatically 
		for you so that you can enjoy a nice coffee when adding or removing a node instead of calculating and 
		assigning new token ranges for each node (which is a very error-prone operation, by the way).
		Well, that said, the partitioner is the component responsible for determining how to distribute the data across the nodes in the cluster given the partition key of a row. Basically, it is a hash function for computing a token given the partition key.
		Once the partitioner applies the hash function to the partition key and gets the token, 
		it knows exactly which node is going to handle the request.
	
	Replication
		Life would be much easier if…
			- Nodes never fail
			- Networks had no latency
			- People did not stumble on cables
			- Amazon did not restart your instances
			- Full GC meant “Full Guitar Concert”
		And so on. Unfortunately, these things happen all the time and you already chose a software engineer career.
		Fortunately, Cassandra offers automatic data replication and keeps your data redundant throughout different nodes in the cluster. 
		This means that (in certain levels) you can even resist to node failure scenarios and your data would still be safe and available. 
		But everything comes at a price, and the price of replication is consistency.

	Replication Strategy
		Basically, the coordinator uses the Replication Strategy to find out which nodes will be the replica nodes for a given request.
		There are two replication strategies available:
			- SimpleStrategy: used for a single data center deployment (not recommended for production environment). 
			  It doesn’t consider the network topology. Basically, it just takes the partitioner’s decision 
			  (that is, the node that will handle the request first based on the token range) 
			  and places the remaining replicas clockwise in relation to this node. 
			- NetworkTopologyStrategy: used for multiple data centers deployment (recommended for production environment). 
			  It also takes the partitioner’s decision and places the remaining replicas clockwise, 
			  but it also takes into consideration the rack and data centers configuration.

	Replication Factor
		When you create a table (Column Family) in Cassandra, you specify the replication factor. 
		The replication factor is the number of replicas that Cassandra will hold for this table in different nodes. 
		If you specify REPLICATION_FACTOR=3, then your data will be replicated to 3 different nodes throughout the cluster. 
		That provides fault tolerance and resilience because even if some nodes fail your data would still be safe and available.

	Write Consistency Level
		Do you still remember that when the client sends a request to a Cassandra node, 
		this node is called a coordinator and acts as a proxy between the client and the replica nodes?
		Well, when you write to a table in Cassandra (inserting data, for example), you can specify the write consistency level. 
		The write consistency level is the number of replica nodes that have to acknowledge the coordinator 
		that its local insert was successful (success here means that the data was appended to the commit log and written to the memtable). 
		As soon as the coordinator gets WRITE_CONSISTENCY_LEVEL success acknowledgments from the replica nodes, 
		it returns success back to the client and doesn’t wait for the remaining replicas to acknowledge success.
		For example, if an application issue an insert request with WRITE_CONSISTENCY_LEVEL=TWO to a table that is configured with REPLICATION_FACTOR=3, the coordinator will only return success to the application when two of the three replicas acknowledge success. 
		Of course, this doesn’t mean that the third replica will not write the data too; it will, but at this point, the coordinator would already have sent success back to the client.
		There are many different types of write consistency levels you can specify in your write request. 
		From the less consistent to full consistency: ANY, ONE, TWO, THREE, QUORUM, LOCAL_QUORUM, EACH_QUORUM, ALL.

	Write Flow Example
		For simplicity, suppose a write request is issued to a 6-node Cassandra cluster with the following characteristics:
			WRITE_CONSISTENCY_LEVEL=TWO
			TABLE_REPLICATION_FACTOR=3
			REPLICATION_STRATEGY=SimpleStrategy
		
		First, the client sends the write request to the Cassandra cluster using the driver. 
		The driver is responsible for a lot of features such as asynchronous IO, parallel execution, request pipelining, connection pooling, 
		auto node discovery, automatic reconnection, token awareness, and so on. 
		
		For example, by using a driver that implements a token-aware policy, the driver reduces network hops by sending requests 
		directly to the node that owns the data instead of sending it to a “random” coordinator.
		As soon as the coordinator gets the write request, it applies the partitioner hash function to the partition key 
		and uses the configured Replication Strategy in order to determine the TABLE_REPLICATION_FACTOR replica nodes 
		that will actually write the data (in this sentence, replace TABLE_REPLICATION_FACTOR with the number 3). 
		
		Now, before the coordinator forwards the write request to all the 3 replica nodes, 
		it will ask to the Failure Detector component how many of these replica nodes are actually available and compare it to the WRITE_CONSISTENCY_LEVEL provided in the request. If the number of replica nodes available is less than 
		the WRITE_CONSISTENCY_LEVEL provided, the Failure Detector will immediately throw an Exception.
		
		For our example, suppose the 3 replica nodes are available (that is, the Failure Detector will allow the request to continue). 
		Now, the coordinator will asynchronously forward the write request to all the replica nodes 
		(in these case, the 3 replica nodes that were figured in the first step). 
		As soon as WRITE_CONSISTENCY_LEVEL replica nodes acknowledge success (node2 and node4), 
		the coordinator returns success back to the driver.

		If the WRITE_CONSISTENCY_LEVEL for this request was THREE (or ALL), the coordinator would have to wait until 
		node3 acknowledges success too, and of course that this write request would be slower.
		So, basically…
			- Do you need fault tolerance and high availability? Use replication.
			- Just bear in mind that using replication means you will pay with consistency (for most of the cases, 
			  this is not a problem. Availability is often more important than consistency).
			- If consistency is not an issue for your domain, perfect. If it is, just increase the consistency level, 
			  but then you will pay with higher latency.
			- If you want fault tolerance and high availability, strong consistency and low latency, 
			  then you should be the client, not the software engineer (Lol).

	Hinted Handoff
		Suppose in the last example that only 2 of 3 replica nodes were available. In this case, the Failure Detector 
		would still allow the request to continue as the number of available replica nodes is not less than the WRITE_CONSISTENCY_LEVEL provided. 
		In this case, the coordinator would behave exactly as described before but there would be one additional step. 
		The coordinator would write locally the hint (the write request blob along with some metadata) in the disk (hints directory) 
		and would keep the hint there for 3 hours (by default) waiting for the replica node to become available again. 
		If the replica node recovers within this period, the coordinator will send the hint to the replica node so that 
		it can update itself and become consistent with the other replicas. If the replica node is offline for more than 3 hours, 
		then a read repair is needed. This process is referred as Hinted Handoff.

	Write Internals
		In short, when a write request reaches a node, mainly two things happen:
			1. The write request is appended to the commit log in the disk. This ensures data durability 
			   (the write request data would permanently survive even in a node failure scenario)
			2. The write request is sent to the memtable (a structure stored in the memory). When the memtable is full, 
			   the data is flushed to a SSTable on disk using sequential I/O and the data in the commit log is purged.

Read Path Anatomy
	
	Steps:
		Cassandra read path is the process followed by a Cassandra node to retrieve data in response to a read operation. 
		The read path has more steps than the write path. Actions performed to serve a read request are as follows:
			1. The coordinator generates a hash using the partition key and gathers the replica nodes which are responsible for storing the data.
			2. The coordinator checks if replicas required to satisfy the read consistency level are available. 
			   If not, an exception is thrown, and the read operation ends.
			3. The coordinator then sends a read data request to the fastest responding replica; 
			   the fastest replica could be the coordinator itself. The fast replica is determined by a dynamic snitch, 
			   which keeps track of node latencies dynamically.
			4. The coordinator then sends a digest request to the replicas of data. 
			   The digest is a hash calculated over requested data by the replica nodes.
			5. The coordinator compares all the digests to determine whether all the replicas have a consistent version of the data. 
			   If those are equal, it returns the result obtained from the fastest replica.
		If the digests from all the replicas are not equal, it means some replicas do not have the latest version of the data. 
		In this case, read data requests for all the replicas are issued, and the data with the latest timestamp is returned to the client. 
		Also, read repair requests are issued for the replicas which do not have the latest data version.

	Components involved in a read operation on a node:

		- Row cache: This is a cache for frequently read data rows, also referred to as hot data. 
		  It stores a complete data row which can be returned directly to the client if requested by a read operation. 
		  This is an optional feature and works best if there are a small number of hot rows which can fit in the row cache.
		
		- Partition key cache: This component caches the partition index entries per table which are frequently used. 
		  In other words, it stores the location of partitions that are commonly queried but not the complete rows. 
		  This feature is used by default in Cassandra, but it can be optimized more.
		
		- Bloom filter: A bloom filter is a data structure that indicates if a data partition could be included in a given SSTable. 
		  The positive result returned by a bloom filter can be a false signal, but the negative results are always accurate. 
		  Hence it saves a lot of seek-time for read operations.

		- Partition index and summary: A partition index contains the offset of all partitions for their location in SSTable. 
		  The partition summary is a summary of the index. These components enable locating a partition exactly in an SSTable 
		  rather than scanning data.
	
		- Memtable: Memtable is an in-memory representation of SSTables. If a data partition is present in memtable, 
		  it can be directly read for specific data rows and returned to the client.
	
		- Compression offset map: This is the map for locating data in SSTables when it is compressed on-disk. 
		
		- SSTable: The on-disk data structure which holds all the data once flushed from memory. 

	Read Operation on a Node

		1. Cassandra checks the row cache for data presence. If present, the data is returned, and the request ends.
		
		2. The flow of request includes checking bloom filters. If the bloom filter indicates data presented in an SSTable, 
		   Cassandra continues to look for the required partition in the SSTable.
		
		3. The key cache is checked for the partition key presence. The cache hit provides an offset for the partition in SSTable. 
		   This offset is then used to retrieve the partition, and the request completes.

		4. Cassandra continues to seek the partition in the partition summary and partition index. 
		   These structures also provide the partition offset in an SSTable which is then used to retrieve the partition and return. 
		   The caches are updated if present with the latest data read.

	Read Repair
		Cassandra ensures that frequently read data remains consistent. Once a read is done, the coordinator node compares the data 
		from all remaining replicas that own the row in the background. If they are inconsistent, issues writes to the 
		out of data replicas to update the row to reflect the most recently written values.
		Read repairs can be configured per column family and is enabled by default.
		Read repair is important because every time a read request occurs, it provides an opportunity for consistency improvement. 
		As a background process, read repair generally puts little strain on the cluster.

		How does a read repair work?
			When a query is made against a given key…
			1. Cassandra performs a Read Repair
			2. Read Repair perform a digest query on all replicas for that key. A digest query asks a replica to 
			   return a hash digest value and the timestamp for the key’s data. Digest query verify whether replica 
			   posses the same data without sending the data over the network.
			3. Cassandra pushes the most recent data to any out of date replicas to make the queries data consistent again. 
			   Next query will therefore return a consistent data.

	Strategies for Reads
		ONE — reads from the closest node holding the data
		QUORUM — returns a result from a quorum of servers with the most recent timestamp of data
		LOCAL_QUORUM — returns a result from a quorum of servers with the most recent timestamp for the data 
					   in the same data center as teh coordinator node
		EACH_QUORUM — returns a result from a quorum of servers with the most recent timestamp in all data centers
		ALL — returns a result from all replica nodes for a row key

Data Replication
	Data replication refers to storing copies of each row in multiple nodes. 
	The reason for data replication is to ensure reliability and fault tolerance. 
	Consequently, if any node fails for any reason, the replication strategy makes sure that the same data is available in other nodes.
	
	The replication factor (RF) specifies how many nodes across the cluster would store the replicas.
	There are two available replication strategies:
		1. The SimpleStrategy is used for a single data center and one rack topology. 
		   First, Cassandra uses partitioner logic to determine the node to place the row. 
		   Then it puts additional replicas on the next nodes clockwise in the ring.

		2. The NetworkTopologyStrategy is generally used for multiple datacenters and multiple racks. 
		   Additionally, it allows you to specify a different replication factor for each data center. 
		   Within a data center, it allocates replicas to different racks to maximize availability.

Consistency Level
	Consistency indicates how recent and in-sync all replicas of a row of data are. 
	With the replication of data across the distributed system, achieving data consistency is a very complicated task.

	Cassandra prefers availability over consistency. It doesn't optimize for consistency. Instead, 
	it gives you the flexibility to tune the consistency depending on your use case. 
	In most use cases, Cassandra relies on eventual consistency.

	Consistency Level (CL) on Write
		For write operations, the consistency level specifies how many replica nodes must acknowledge back before the coordinator successfully reports back to the client. More importantly, the number of nodes that acknowledge (for a given consistency level) and the number of nodes storing replicas (for a given RF) are mostly different.

		For example, with the consistency level ONE and RF = 3, even though only one replica node acknowledges back for a successful write operation, Cassandra asynchronously replicates the data to 2 other nodes in the background.

		The consistency level ONE means it needs acknowledgment from only one replica node. Since only one replica needs to acknowledge, the write operation is fastest in this case.

		The consistency level QUORUM means it needs acknowledgment from 51% or a majority of replica nodes across all datacenters.

		The consistency level of LOCAL_QUORUM means it needs acknowledgment from 51% or a majority of replica nodes just within the same datacenter as the coordinator. Thus, it avoids the latency of inter-datacenter communication.

		The consistency level of ALL means it needs acknowledgment from all the replica nodes. Since all replica nodes need to acknowledge, the write operation is the slowest in this case. Moreover, if one of the replica nodes is down during the write operation, it fails, and availability suffers. Therefore, the best practice is not to use this option in production deployment.

		We can configure the consistency level for each write query or at the global query level.

		The diagram [consistency-level-write.webp] shows a couple of examples of CL on write:

	Consistency Level (CL) on Read
		For read operations, the consistency level specifies how many replica nodes must respond with the latest consistent data before the coordinator successfully sends the data back to the client.

		The consistency level ONE means only one replica node returns the data. The data retrieval is fastest in this case.

		The consistency level QUORUM means 51% or a majority of replica nodes across all datacenters responds. 
		Then the coordinator returns the data to the client. In the case of multiple data centers, 
		the latency of inter-data center communication results in a slow read.

		The consistency level of LOCAL_QUORUM means 51% or a majority of replica nodes within the same datacenter. 
		As the coordinator responds, then the coordinator returns the data to the client. Thus, it avoids the latency of inter-datacenter communication.

		The consistency level of ALL means all the replica nodes respond, then the coordinator returns the data to the client. Since all replica nodes need to acknowledge, the read operation is the slowest in this case. Moreover, if one of the replica nodes is down during the read operation, it fails, and availability suffers. The best practice is not to use this option in production deployment.

		We can configure the consistency level for each read query or at the global query level.

		The diagram [consistency-level-read.webp] shows a couple of examples of CL on read:

Repair
	Cassandra is designed to remain available if one of it’s nodes is down or unreachable. 
	However, when a node is down or unreachable, it needs to eventually discover the writes it missed. 
	Hints attempt to inform a node of missed writes, but are a best effort, 
	and aren’t guaranteed to inform a node of 100% of the writes it missed. 
	These inconsistencies can eventually result in data loss as nodes are replaced or tombstones expire.

	These inconsistencies are fixed with the repair process. Repair synchronizes the data between nodes by 
	comparing their respective datasets for their common token ranges, and streaming the differences for 
	any out of sync sections between the nodes. It compares the data with merkle trees, which are a hierarchy of hashes.

	Incremental and Full Repairs
		There are 2 types of repairs: full repairs, and incremental repairs. 
			1. Full repairs operate over all of the data in the token range being repaired. 
			2. Incremental repairs only repair data that’s been written since the previous incremental repair.

		Incremental repairs are the default repair type, and if run regularly, can significantly reduce the time 
		and io cost of performing a repair. However, it’s important to understand that once an incremental repair 
		marks data as repaired, it won’t try to repair it again. This is fine for syncing up missed writes, 
		but it doesn’t protect against things like disk corruption, data loss by operator error, or bugs in Cassandra. 
		For this reason, full repairs should still be run occasionally.

	To orchestrate the repairs I’m gonna use Cassandra-Reaper tool which built and open sourced by Spotify. 
	Now this package is maintaining by a team at Thelastpickle. 

	In some scenarios Cassandra automatically repair the data inconsistencies with Hinted Hand-off and Read-Repair. 
	But in some scenarios such as node goes down longer than max_hint_window_in_ms(which defaults to 3 hours), 
	having low read write consistency levels, corrupted SSTables, more manual data deletion with out using TTLs, 
	low time interval in TTL(lower than 3 hours) we need to run manual repair on Cassandra cluster. 

	The manual repair process in Cassandra known as Anti-Entropy repair. Anti-Entropy repair in Cassandra is a 
	maintenance operation that restores data consistency throughout a cluster(data inconsistencies are fixed 
	with the repair process). Repair synchronizes the data between nodes by comparing their respective datasets 
	for their common token ranges, and streaming the differences for any out of sync sections between the nodes. 
	It is advised to run repair operations at leasts every gc_grace_seconds(default 10 days) to ensure 
	that tombstones will get replicated consistently to avoid zombie records if you perform DELETE statements on your tables.

	How repair works
		The Anti-Entropy repair perform repairs without comparing all data between all replicas, 
		Cassandra uses Merkle trees to compare trees of hashed values instead. We can run the repair with nodetool reapir command. 
		It can be run on either a specified node or on all nodes(if a node is not specified it runs on all nodes). 
		The node that initiates the repair becomes the coordinator node for the operation. Following are the steps of repairing.

			1. Coordinator node scans the partition ranges that it owns (either as primary or replica) one by one. 
			For each partition range, it sends the request to each of the peer/replica nodes to build a Merkle tree.
			
			2. The peer/replica node scans all SSTables and a major, or validation, compaction is triggered. 
			It reads every row in the SSTables, generates a hash for it, and then adds the result to a Merkle tree.
			
			3. Once the peer node finishes building the Merkle tree, it sends the result back to the coordinator. 
			The coordinator node compares every Merkle tree with all other trees. If difference is detected, data is exchanged between differing nodes.

	Concerns in reapair
		1. Overstreaming - In Cassandra Merkle trees can have maximum 32,000 leaf nodes. Due to this reason each leaf node of the Merkle tree may not responsible for one row in the data set. It may contains thousand of rows(with GBs of size). If data mismatch occurs in one record in the leaf node, it needs to stream whole leaf node between peers/replicas instead of streaming single mismated record. It could causes streaming large data between nodes(could be GBs or TBs of size) which identified as overstreaming.
		
		2. High CPU/IO operations - Building Merkle tree requires hashing every row of all SSTables, it is a very expensive operation, stressing CPU, memory, and disk I/O.
		
		3. Long time and stuck repairs - Based on the data load in the nodes, repair may take long time. Sometimes with stuck behaviors.
		
		4. Compaction bottleneck - Building Merkle trees and exchanging Merkle trees deal with validation compaction. Based on the repair strategy and data load bottlenecks can be occurred.

	Improve repairs
		Cassandra nodetool introduced some mechanisms to provide better repair strategies to run repairs effectively and smoothly.
		
		1. Primary range repair - By default, repair will operate on all token ranges replicated by the node you’re running the repair. It will cause duplicate work if you run the repair on every node. Primary range repair will only repairs the data of the primary range, but not other ranges managed on the node. When using this option, it avoids the cost of doing Merkle tree calculation on non-primary range data. It also helps reduce excessive data streaming across the network. Since each node only repairs one range of data that is managed by a node, this option needs to run on ALL nodes in the ring in order to repair all the data. This option is available in very early release (0.x) of Cassandra.

		2. Parallel/Sequential repair - With parallel repairs, the expensive repair operations (especially the Merkle tree building part) can run on multiple nodes concurrently. This could be problematic and may slow down the entire cluster. With sequential repairs at any time for a given replica set of data, only one replica is being repaired, allowing other replica to satisfy the application requests in a more performant way. Sequential is a default strategy after Cassandra 2.0.

		3. Incremental repair - Normal repairs which identifies as full repairs build Merkel trees from all SSTables(repaired or not). To overcome this concerns incremental repairs has been introduced in Cassandra 2.1. This has become the default strategy since Cassandra 2.2. Instead of building a Merkle tree out of all SSTables (repaired or not), incremental option only builds the tree out of un-repaired SSTables. To do this it maintains two SSTables(one that contains the data that was repaired in the session and another one with the remaining un-repaired data). With this option the size of the Merkel tree will be reduced and cost less CPU/IO resources and compaction bottleneck, less data will be streamed over the network, already repaired data won’t be repaired again. Incremental repair is not concurrency oriented. If we run incremental repairs concurrently in multiple nodes, it may causes errors.

		4. Subrange repair - To solve the issues with overstreaming subrange repair can be used. Subrange repair involves repairing smaller subset of the data in token range of a node. It has options of -st (or –start-token) and -et(or –end-token) to specify a particular sub-range of data to repair. Subrange repairs can be parallelly run on different sets of data at the same time, either on the same node, or on different nodes. This repairs are incompatible with incremental repairs, that means subrange repairs cannot run as incrementally. Subrange repairs allows DevOps teams to schedule the repairs to best match the cluster’s workload. Any way need to make sure that all subrange repairs run with in gc_grace_seconds limit.

	When and when not to repair
		When and when not to run repairs in Cassandra clusters? is always a debatable question. 
	
		Following are some common scenarios which repairs are required.
		
		1. Multi region clusters.
		2. Low consistent read and writes.
		3. Frequent node outages.
		4. Low read-repair change.
		5. Having flaky, easily breakable network.

		Following are some scenarios which may not need to run repairs in Cassandra cluster.
	
		1. Short TTLs.
		2. Just do single writes with no updates.
		3. Have a single data center and rely on TTLs to delete your data
		4. Highly consistent read and writes.
		5. High read repair chance.

	Repair best practices
		Following are some best practices we can follow and some bad practices we need to avoid when running Cassandra repairs.

		1. Avoid running repairs on all nodes at once in the cluster since it could lead performance bottleneck.
		
		2. Put your repair strategy in place from day one when deploying the cluster. Don’t wait till getting errors.
		
		3. Use tools to orchestrate the repairs. Build your own tool or use some available tools like OpsCenter(available on
		   datastax enterprise) and Cassandra-Reaper.

		4. Complete whole repair cycles with in gc_grace_seconds limit(run repair at lease once in gc_grace_seconds limit).

		5. Don’t repair everything in the cluster, choose tables with more manual DELETE operations and critical data.

		6. Avoid concurrency in incremental repairs(run one node at a time).

	Repair orchestration with Reaper
		When running repairs in larger clusters continuously, we need to have some way to orchestrate the repairs. 
		There are some tools available to do repair orchestration such as OpsCenter(in datastax enterprise) and Cassandra-Reaper. 

		About Reaper
			Reaper is an open source tool that aims to schedule and orchestrate repairs of Apache Cassandra clusters. 
			It improves the existing nodetool repair process by 

			1. Splitting repair jobs into smaller tunable segments.
			2. Handling back-pressure through monitoring running repairs and pending compactions.
			3. Adding ability to pause or cancel repairs and track progress precisely.

		Reaper ships with a REST API, a command line tool and a web UI. It connects to Cassandra cluster via JMX endpoint and executes nodetool commands. The latest Reaper version(2.0) allows to run Reaper as sidecar on Cassandra cluster.

Compaction
	Cassandra is optimized for writing large amounts of data very quickly. 
	Cassandra writes all incoming data in an append-only manner in internal files called SSTables. 
	These SSTables hence contain newly inserted data and updates/deletes of previously inserted data.

	Cassandra Compaction is a process of reconciling various copies of data spread across distinct SSTables. 
	Cassandra performs compaction of SSTables as a background activity. 
	Cassandra has to maintain fewer SSTables and fewer copies of each data row due to compactions improving its read performance.
	Compaction is a crucial area of Cassandra performance and maintenance.

	While regular compactions are an integral part of any healthy Cassandra cluster – the way that they are configured 
	can vary significantly depending on the way a particular table is being used. 
	It is important to take some time when designing a Cassandra table schema to think about 
	how each table will be used, and therefore the Cassandra Compaction strategy would be most effective.

	Although it is possible to change compaction strategies after the table has been created, 
	doing so will have a significant impact on the cluster’s performance that is proportionate 
	to the amount of data in the table. This is because Cassandra will re-write all of the data 
	in that table using the new Compaction Strategy.

	To understand the importance of compactions in Cassandra, you must first understand how Cassandra writes data to a disk. 
	The Cassandra write path in a nutshell:

		1. Cassandra stores recent writes in memory (in a structure called the Memtable).
		
		2. When enough writes have been made, Cassandra flushes the Memtable to disk. 
		   Data on disk is stored in relatively simple data structures called Sorted String Tables (SSTable). 
		   At the most simplified level, an SSTable could be described as a sorted array of strings.

		3. Before writing a new SSTable, Cassandra merges and pre-sorts the data in the Memtable according to Primary Key. 
		   In Cassandra, a Primary Key consists of a Partition Key (the unique key that determines which node 
		   the data is stored on) and any Clustering Keys that have been defined.

		4. The SSTable is written to disk as a single contiguous write operation. 
		   SStables are immutable. Once they are written to disk they are not modified. 
		   Any updates to data or deletion of data within an SSTable is written to a new SSTable. 
		   If data is updated regularly, Cassandra may need to read from multiple SSTables to retrieve a single row.
		
		5. Compaction operations occur periodically to re-write and combine SSTables. 
		   This is required because SSTables are immutable (no modifications once written to disk). 
		   Compactions prune deleted data and merge disparate row data into new SSTables in order 
		   to reclaim disk space and keep read operations optimized.

	Compaction Strategies
		Multiple Compaction Strategies are included with Cassandra, and each is optimized for a different use case:
		
		Type of Compaction Strategy
			SizeTiered Compaction Strategy (STCS)

		Description
			This is the default compaction strategy. This compaction strategy triggers a compaction 
			when multiple SSTables of a similar size are present. 
			Additional of parameters allow STCS to be tuned to increase or decrease 
			the number of compactions it performs and how tombstones are handled.

		When
			This compaction strategy is good for insert-heavy and general workloads.

		------------------------------------------------------------------------------------------------------------

		Type of Compaction Strategy
			Leveled Compaction Strategy (LCS)
		
		Description
			This strategy groups SSTables into levels, each of which has a fixed size limit which is 10 times 
			larger than the previous level. SSTables are of a fixed, relatively small size (160MB by default) – 
			so if Level 1 might contain 10 SSTables at most, then Level 2 will contain 100 SSTables at most. 
			SSTables are guaranteed to be non-overlapping within each level – 
			if any data overlaps when a table is promoted to the next level, overlapping tables are re-compacted.

			For example: when Level 1 is filled, any new SSTables being added to that level are compacted 
			together with any existing tables that contain overlapping data. 
			If these compactions result in Level 1 now containing too many tables, 
			the additional table(s) overflow to Level 2.

		When
			This compaction strategy is the best for read-heavy workloads (because tables within a level 
			are non-overlapping, LCS guarantees that 90% of all reads can be satisfied from a single SSTable) 
			or workloads where there are more updates than there are inserts.

		------------------------------------------------------------------------------------------------------------

		Type of Compaction Strategy
			DateTiered Compaction Strategy (DTCS)

		Description
			This compaction strategy is designed for use with time-series data. 
			DTCS stores data written within a the same time period in the same SSTable. 
			Multiple SSTables that are themselves written in the same time window will be compacted together, 
			up until a certain point, after which the SSTables are no longer compacted. 
			SSTables are also configured with a TTL. SSTables that are older than the TTL will be dropped, 
			incurring zero compaction overhead.

		When
			DTCS is deprecated in Cassandra 3.0.8/3.8 and later. 
			The TWCS is improved version of DTCS, it is available with version 3.0.8/3.8 and later.
			
			DTCS is highly performant and efficient, but only if the workload matches the strict requirements of DTCS. 
			DTCS is not designed to be used with workloads where there are updates to old data or 
			inserts that are out of order. If your workload does not fit these requirements, 
			you may be better off using STCS and using a bucketing key (such as hour/day/week) to break up your data.
		
		------------------------------------------------------------------------------------------------------------

		Type of Compaction Strategy
			Time Window Compaction Strategy

		Description
			The Time Window Compaction Strategy is designed to work on time series data. 
			It compactes SSTables within a configured time window. 
			TWCS utilizes STCS to perform these compactions. At the end of each time window, 
			all SSTables are compacted to a single SSTable so there is one SSTable for a time window. 
			TWCS also effectively purges data when configured with time to live by dropping complete SSTables after TTL expiry.
			
			TWCS is similar to DTCS except for operational improvements. 
			Write amplifications is prevented by only compacting SSTables within a time window. 
			The maximum SSTable timestamp is used to decide which time window it belongs to.

		When
			TWCS is ideal for time series data which is immutable after a fixed time interval. 
			Updating the data after the specified time window results in multiple SSTables referring to the same data. 
			Those SSTables will not get compacted together.
 			
 			TWCS can be used with Cassandra 2.x by  adding jar file.

 Anti-Entropy
 	in systems without strong coordination, consistency guarantees are somewhat loosened. 
 	In order to keep nodes in sync in presence of network partitions, anti-entropy protocols 
 	are used in order to “repair” the data and reconcile missing records.

	Merkle-Trees 
	 	are a binary hash-tree, holding hierarchies of hashes, often used for conflict resolution. 
	 	Each inner node stores the hash of its children hashes; 
	 	the leaf nodes store a list of key-hash pairs. By comparing hashes from top to the bottom, 
	 	it is possible to locate the difference between trees and, subsequently, the information they represent.

	Causal Consistency
		a model that captures causal relationship between two operations and guarantees that 
		all processes can observe these operations in common causal order. 
		This is an important model in distributed systems, since even perfect physical clocks, 
		which cannot be achieved in practice, will still fail to capture the causality between updates.

	Distributed Deletes
		given a delete request, completely removing an object from storage is normally not possible 
		without losing causality information which may lead to that object resurfacing 
		via delayed replication messages or synchronisation with outdated nodes. 
		In order to solve this problem, tombstones serve as placeholders for a key, indicating that the record was removed.

	Tips:
		Every node keeps the log of operations, occurred locally or replicated, per peer. 
		During anti-entropy, a node compares it’s own local Node Clock 
		(represented in a compact way as a Bitmapped Version Vector) with remote Node Clock.
		Node Clock summaries the local storage history, causality metadata for each object version. 
		It represents which update events this node has seen, directly (coordinated by itself) 
		or transitively (received from others). Concretely, the node clock groups dots per peer node, 
		factoring out the node id part from the dots. For each node, the node clock represents 
		the set of counters in two parts: 
			an base counter representing the contiguous sequence starting from 1 (as in Version Vectors), 
			and a set of non-contiguous counters. 
		It can be thought of as indicated [node-logical-clock-bitmap.png]

		In order to map dots to the actual keys they represent, a special Dot-Key map is stored. 
		So when a certain dots are missing during repair or anti-entropy process, they’re filled using this map.

		In order to keep the clocks compact, node base versions are “stripped” and replicated versions 
		are removed from causal context.

Tombstones
	Deleting data is not the same in Cassandra as it is with a relational database. 
	Unlike a relational database system, Cassandra does not remove the data immediately 
	but simply captures the delete operation as a marker on that data which is called a Tombstone. 
	This very fact presents the challenge of propagating deletes on the stored data. 
	If not properly handled, this can become a performance bottleneck, 
	therefore it is crucial to understand how deletes are handled in Cassandra.

	The Need for Tombstones
		Apache Cassandra is a distributed database that offers high availability and partition tolerance 
		with eventual or tunable consistency. It uses log-structured merge-tree storage that means, 
		writes are always appended while reads take care of merging multiple fragments of a row by 
		picking the latest version of data for each column. Any node in a Cassandra cluster can process 
		a write operation and every write is sent to all replica nodes for that data. 
		Now consider a situation when one of the replica nodes was down when a delete operation was performed; 
		the node would simply miss the delete request. Once this downed node comes back online, 
		it would mistakenly think that all the other nodes where the delete was applied 
		had actually missed a write and would start repairing all the other nodes by sending the deleted data. 
		This would lead to data resurrection issues in the cluster. 
		Therefore in-place deletes will not work with a distributed system like Cassandra and 
		would need a more sophisticated mechanism to handle deletes called Tombstones.

		A Tombstone is a special marker for the data by which Cassandra creates a placeholder for the deleted data. 
		What this means is, data is not immediately deleted from your data store, 
		so you wouldn’t see the size of your data store shrink immediately following a delete operation. 
		If any of the nodes did not receive the delete request, the tombstone can be replayed to such nodes 
		when they are back online again. Each node keeps track of the age of all its tombstones. 
		A configurable parameter gc_grace_seconds is the time for which tombstones will be retained 
		by a node before a compaction cycle runs and the tombstones are garbage collected 
		freeing up the disk space on the node. The default value for gc_grace_seconds is 864,000 (or 10 days).

	Tombstones and reads
		Every piece of data that is written to Cassandra is stored with an associated timestamp attached to it, 
		this is applicable even for tombstones. Tombstones are scanned by Cassandra while servicing a read request. 
		The shards of the data from memtable and SSTables are merged together along with tombstones and 
		the correct data value is chosen with the Last-Write-Wins (LWW) algorithm. 
		This means, your read performance will degrade with the number of tombstones present in your tables.

	Types of tombstones
		Tombstones can get created in a number of ways and it is crucial that you understand certain pitfalls 
		that can lead to implicit creation of tombstones that will remain hidden from the programmer’s point of view 
		until it surfaces as a plausible issue affecting your cluster. 
		Therefore, it’s important to understand the types of tombstones that can be created in Cassandra:

		Cell tombstones
			Insert statements can create tombstones when a certain cell value is set as null in the query. 
			This can happen when the database abstraction layer or an ORM framework abstracts the query 
			with object-level representation and the null values get implicitly sent down in the actual query to Cassandra. 
			For instance, consider the following CQL query:

				INSERT INTO item_price 
					( store_number,  item_id  , price  , replacements , product_code ) 
				VALUES 
					( ‘CA104’     , ‘item104’ , 2.50   , null         , ‘p104’);

			This would create a cell tombstone for the replacements column for the record with store_number CA104.

			Now consider the following delete query:
			
				DELETE replacements FROM item_price WHERE store_number = ‘CA104’;

			This would also create a cell tombstone for the corresponding record.

		Row tombstones
			An entire row is marked as a tombstone as a result of a delete query that identifies a row. For example:

			DELETE FROM item_price WHERE store_number = ‘CA101’ and item_id=’item101' and price = 1.80;

			Sstabledump would show a deletion_info at the row level for the clustering columns within the partition.

			A large number of row tombstones can be an indication of a poor data model whereby your application 
			is frequently deleting records from a table. In such cases, consider revisiting your data model 
			and redesigning tables based on query patterns and the cardinality.


		Range tombstones
			Deleting an entire range of rows using WHERE clause with a partition key and a range represented 
			by a clustering column. For instance:

			DELETE FROM item_price WHERE store_number = ‘CA101’ AND item_id=’item101' AND price > 2.0;

		Partition tombstones
			Tombstones of this type are created when a delete query is fired using only the partition key in the WHERE clause. 
			For example:

				DELETE FROM item_price WHERE store_number = ‘CA102’;

			As you have undoubtedly guessed, this would delete the entire partition CA102 and the sstabledump 
			would show the partition the deletion_info attribute with marked_deleted timestamp.

		TTL tombstones
			These are tombstones created automatically when the time-to-live expires for a particular row or cell. 
			However, they are marked differently than normal tombstones.
			The following insert statement would create a TTL tombstone after 20 seconds.

			INSERT INTO item_price 
				( store_number, item_id  , price , replacements             , product_code) 
			VALUES 
				( ‘CA103’     , ‘item103’, 3.0   , {‘item101-r’, ‘item101’} , ‘p103’) 
			using TTL 20;

