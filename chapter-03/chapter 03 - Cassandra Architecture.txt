Architecture Requirements of Cassandra
	Cassandra was designed to address many architecture requirements. 
	The most important requirement is to ensure there is no single point of failure. 
	Its architecture is based on the understanding that system and hardware failures can and do occur.
	This means that if there are 100 nodes in a cluster and a node fails, the cluster should continue to operate.
	Cassandra addresses the problem of failures by employing a peer-to-peer distributed system 
	across homogeneous nodes where data is distributed among all nodes in the cluster.
	Each node frequently exchanges state information about itself and other nodes across the cluster using peer-to-peer gossip communication protocol. 

	This is in contrast to Hadoop where the namenode failure can cripple the entire system. 
	Another requirement is to have massive scalability so that a cluster can hold hundreds or thousands of nodes. 
	It should be possible to add a new node to the cluster without stopping the cluster.

	Further, the architecture should be highly distributed so that both processing and data can be distributed. 
	Also, high performance of read and write of data is expected so that the system can be used in real-time. 
	Let us explore the Cassandra architecture in the next section of the cassandra architecture tutorial.
	
	A sequentially written commit log on each node captures write activity to ensure data durability. 
	Data is then indexed and written to an in-memory structure, called a memtable, which resembles a write-back cache. 
	Each time the memory structure is full, the data is written to disk in an SSTables data file. 
	All writes are automatically partitioned and replicated throughout the cluster. 
	Cassandra periodically consolidates SSTables using a process called compaction, 
	discarding obsolete data marked for deletion with a tombstone. 
	To ensure all data across the cluster stays consistent, various repair mechanisms are employed.
	
	Client read or write requests can be sent to any node in the cluster. 
	When a client connects to a node with a request, that node serves as the coordinator for that particular client operation. 
	The coordinator acts as a proxy between the client application and the nodes that own the data being requested. 
	The coordinator determines which nodes in the ring should get the request based on how the cluster is configured.
	
	Commit log
		All data is written first to the commit log for durability. After all its data has been flushed to SSTables, it can be archived, deleted, or recycled. 
	
	SSTable
		A sorted string table (SSTable) is an immutable data file to which Cassandra writes memtables periodically. 
		SSTables are append only and stored on disk sequentially and maintained for each Cassandra table.


Key components for configuring cassandra
	
	Gossip
		A peer-to-peer communication protocol to discover and share location and state information about the other nodes in a Cassandra cluster. 
		Gossip information is also persisted locally by each node to use immediately when a node restarts.
		
	Partitioner
		A partitioner determines which node will receive the first replica of a piece of data, 
		and how to distribute other replicas across other nodes in the cluster. 
		Each row of data is uniquely identified by a primary key, which may be the same as its partition key, 
		but which may also include other clustering columns. 
		
		A partitioner is a hash function that derives a token from the primary key of a row. 
		The partitioner uses the token value to determine which nodes in the cluster receive the replicas of that row. 
		
		The Murmur3Partitioner is the default partitioning strategy for new Cassandra clusters and the right choice for new clusters in almost all cases.
		
		You must set the partitioner and assign the node a num_tokens value for each node. 
		The number of tokens you assign depends on the hardware capabilities of the system. 
		If not using virtual nodes (vnodes), use the initial_token setting instead.
	
	Replication factor
		The total number of replicas across the cluster. 
		A replication factor of 1 means that there is only one copy of each row on one node. 
		A replication factor of 2 means two copies of each row, where each copy is on a different node. 
		All replicas are equally important; there is no primary or master replica. 
		You define the replication factor for each datacenter. 
		Generally you should set the replication strategy greater than one, but no more than the number of nodes in the cluster.
	
	Replica placement strategy
		Cassandra stores copies (replicas) of data on multiple nodes to ensure reliability and fault tolerance. 
		A replication strategy determines which nodes to place replicas on. The first replica of data is simply the first copy; 
		it is not unique in any sense. The NetworkTopologyStrategy is highly recommended for most deployments 
		because it is much easier to expand to multiple datacenters when required by future expansion.

		When creating a keyspace, you must define the replica placement strategy and the number of replicas you want.
		
	Snitch
		A snitch defines groups of machines into datacenters and racks (the topology) that the replication strategy uses to place replicas.
		You must configure a snitch when you create a cluster. All snitches use a dynamic snitch layer, 
		which monitors performance and chooses the best replica for reading. 
		It is enabled by default and recommended for use in most deployments. 
		Configure dynamic snitch thresholds for each node in the cassandra.yaml configuration file.

		The default SimpleSnitch does not recognize datacenter or rack information. 
		Use it for single-datacenter deployments or single-zone in public clouds. 
		The GossipingPropertyFileSnitch is recommended for production. 
		It defines a node's datacenter and rack and uses gossip for propagating this information to other nodes.
	
Cassandra Architecture	
    - Cassandra is designed such that it has no master or slave nodes.
    - It has a ring-type architecture, that is, its nodes are logically distributed like a ring.
    - Data is automatically distributed across all the nodes.
    - Similar to HDFS, data is replicated across the nodes for redundancy.
    - Data is kept in memory and lazily written to the disk.
    - Hash values of the keys are used to distribute the data among nodes in the cluster.
	- Cassandra architecture supports multiple data centers.
	- Data can be replicated across data centers.
	
Effects of the Architecture
	Cassandra architecture enables transparent distribution of data to nodes. 
	This means you can determine the location of your data in the cluster based on the data. 
	Any node can accept any request as there are no masters or slaves. 
	If a node has the data, it will return the data. Else, it will send the request to the node that has the data.

	You can specify the number of replicas of the data to achieve the required level of redundancy. 
	For example, if the data is very critical, you may want to specify a replication factor of 4 or 5.
	If the data is not critical, you may specify just two. 
	
	It also provides tunable consistency, that is, the level of consistency can be specified as a trade-off with performance. 
	Transactions are always written to a commit log on disk so that they are durable. 
	
Cassandra Write Process
	The Cassandra write process ensures fast writes. Steps in the Cassandra write process are:
	
	1. Data is written to a commitlog on disk.
    2. The data is sent to a responsible node based on the hash value.
    3. Nodes write data to an in-memory table called memtable.
    4. From the memtable, data is written to an sstable in memory. 
	   Sstable stands for Sorted String table. This has a consolidated data of all the updates to the table.
    5. From the sstable, data is updated to the actual table.
    6. If the responsible node is down, data will be written to another node identified as tempnode. 
	The tempnode will hold the data temporarily till the responsible node comes alive.

	The diagram below [CassandraArchitecture_3.avif] depicts the write process when data is written to table A.
	
	Data is written to a commitlog on disk for persistence. It is also written to an in-memory memtable. 
	Memtable data is written to sstable which is used to update the actual table.
	
Rack
	The term ‘rack’ is usually used when explaining network topology. 
	A rack is a group of machines housed in the same physical box. 
	Each machine in the rack has its own CPU, memory, and hard disk. 
	However, the rack has no CPU, memory, or hard disk of its own.[CassandraArchitecture_4.avif]
	
	Features of racks are:
		- All machines in the rack are connected to the network switch of the rack
		- The rack’s network switch is connected to the cluster.
		- All machines on the rack have a common power supply. 
		  It is important to notice that a rack can fail due to two reasons: a network switch failure or a power supply failure.
		- If a rack fails, none of the machines on the rack can be accessed. So it would seem as though all the nodes on the rack are down.
	
Cassandra Read Process
	The Cassandra read process ensures fast reads. Read happens across all nodes in parallel. 
	If a node is down, data is read from the replica of the data. 
	Priority for the replica is assigned on the basis of distance. Features of the Cassandra read process are:

    - Data on the same node is given first preference and is considered data local.
    - Data on the same rack is given second preference and is considered rack local.
    - Data on the same data center is given third preference and is considered data center local.
    - Data in a different data center is given the least preference.

	Data in the memtable and sstable is checked first so that the data can be retrieved faster if it is already in memory.
	
Data Partitions
	Cassandra performs transparent distribution of data by horizontally partitioning the data in the following manner:
	- A hash value is calculated based on the primary key of the data.
    - The hash value of the key is mapped to a node in the cluster
    - The first copy of the data is stored on that node.
    - The distribution is transparent as you can both calculate the hash value and determine where a particular row will be stored.

	The following diagram depicts a four node cluster with token values of 0, 25, 50 and 75.[CassandraArchitecture_7.avif]
	
	For a given key, a hash value is generated in the range of 1 to 100. Keys with hash values in the range 
	1 to 25 are stored on the first node, 
	26 to 50 are stored on the second node, 
	51 to 75 are stored on the third node, and 
	76 to 100 are stored on the fourth node. 
	Please note that actual tokens and hash values in Cassandra are 127-bit positive integers.
	
Replication in Cassandra
	Replication refers to the number of replicas that are maintained for each row. 
	Replication provides redundancy of data for fault tolerance. 
	A replication factor of 3 means that 3 copies of data are maintained in the system.
	In this case, even if 2 machines are down, you can access your data from the third copy. 
	
	The default replication factor is 1. A replication factor of 1 means that a single copy of the data is maintained, 
	so if the node that has the data fails, you will lose the data.

	Cassandra allows replication based on nodes, racks, and data centers, 
	unlike HDFS that allows replication based on only nodes and racks. 
	Replication across data centers guarantees data availability even when a data center is down.
	
Network Topology
	Network topology refers to how the nodes, racks and data centers in a cluster are organized. 
	You can specify a network topology for your cluster as follows:
    - Specify in the Cassandra-topology.properties file.
    - Your data centers and racks can be specified for each node in the cluster.
    - Specify <ip-address>=<data center>:<rack name>.
    - For unknown nodes, a default can be specified.
    - You can also specify the hostname of the node instead of an IP address.

	An example of a topology configuration file.[CassandraArchitecture_8.avif]
	This file shows the topology defined for four nodes. The node with IP address 192.168.1.100 is mapped to data center DC1 and is present on the rack RAC1. 
	The node with IP address 192.168.2.200 is mapped to data center DC2 and is present on the rack RAC2.
	Similarly, the node with IP address 10.20.114.10 is mapped to data center DC2 and rack RAC1 and 
	the node with IP address 10.20.114.11 is mapped to data center DC2 and rack RAC1. 
	There is also a default assignment of data center DC1 and rack RAC1 so that any unassigned nodes will get this data center and rack.
	
Snitches
	Snitches define the topology in Cassandra. A snitch defines a group of nodes into racks and data centers. Two types of snitches are most popular:
	1. Simple Snitch - A simple snitch is used for single data centers with no racks.
    2. Property File Snitch - A property file snitch is used for multiple data centers with multiple racks.
	Replication in Cassandra is based on the snitches.
	
Gossip Protocol
	Cassandra uses a gossip protocol to communicate with nodes in a cluster. 
	It is an inter-node communication mechanism similar to the heartbeat protocol in Hadoop. 
	Cassandra uses the gossip protocol to discover the location of other nodes in the cluster and get state information of other nodes in the cluster.
	The gossip process runs periodically on each node and exchanges state information with three other nodes in the cluster. 
	Eventually, information is propagated to all cluster nodes. 
	Even if there are 1000 nodes, information is propagated to all the nodes within a few seconds.

	The following image depicts the gossip protocol process.[CassandraArchitecture_9.avif]
	In step 1, one node connects to three other nodes. 
	In step 2, each of the three nodes connects to three other nodes, thus connecting to nine nodes in total in step 2. 
	So a total of 13 nodes are connected in 2 steps.
	
Seed Nodes
	Seed nodes are used to bootstrap the gossip protocol. The features of seed nodes are:

    - They are specified in the configuration file Cassandra.yaml.
    - Seed nodes are used for bootstrapping the gossip protocol when a node is started or restarted.
    - They are used to achieve a steady state where each node is connected to every other node but are not required during the steady state.

	The diagram[CassandraArchitecture_10.avif] depicts a startup of a cluster with 2 seed nodes. 
	Initially, there is no connection between the nodes. On startup, two nodes connect to two other nodes that are specified as seed nodes. 
	Once all the four nodes are connected, seed node information is no longer required as steady state is achieved.
	
Virtual Nodes
	Virtual nodes in a Cassandra cluster are also called vnodes. 
	Vnodes can be defined for each physical node in the cluster. 
	Each node in the ring can hold multiple virtual nodes. By default, each node has 256 virtual nodes.
	
	Virtual nodes help achieve finer granularity in the partitioning of data, 
	and data gets partitioned into each virtual node using the hash value of the key. 
	On adding a new node to the cluster, the virtual nodes on it get equal portions of the existing data. 
	So there is no need to separately balance the data by running a balancer. 
	The image[CassandraArchitecture_11.avif] depicts a cluster with four physical nodes.
	Each physical node in the cluster has four virtual nodes. So there are 16 vnodes in the cluster.
	If 32TB of data is stored on the cluster, each vnode will get 2TB of data to store. 
	If another physical node with 4 virtual nodes is added to the cluster, 
	the data will be distributed to 20 vnodes in total such that each vnode will now have 1.6 TB of data.
	
Failure Scenarios

	Node Failure
		Cassandra is highly fault tolerant. The effects of node failure are as follows:
		- Other nodes detect the node failure.
		- Request for data on that node is routed to other nodes that have the replica of that data.
		- Writes are handled by a temporary node until the node is restarted.
		- Any memtable or sstable data that is lost is recovered from commitlog.
		- A node can be permanently removed using the nodetool utility
		The following image [CassandraArchitecture_13.avif] shows the concept of node failure.
		
	Disk Failure
		When a disk becomes corrupt, Cassandra detects the problem and takes corrective action. The effects of Disk Failure are as follows:
		- The data on the disk becomes inaccessible
		- Reading data from the node is not possible
		- This issue will be treated as node failure for that portion of data
		- Memtable and sstable will not be affected as they are in-memory tables
		- Commitlog has replicas and they will be used for recovery
		
	Rack Failure
		Sometimes, a rack could stop functioning due to power failure or a network switch problem. The effects of Rack Failure are as follows:
		- All the nodes on the rack become inaccessible
		- Reading data from the rack nodes is not possible
		- The reads will be routed to other replicas of the data
		- This will be treated as if each node in the rack has failed
		The following figure [CassandraArchitecture_14.avif] shows the concept of rack failure:
		
	Data Center Failure
		Data center failure occurs when a data center is shut down for maintenance or when it fails due to natural calamities. When that happens:
		- All data in the data center will become inaccessible.
		- All reads have to be routed to other data centers.
		- The replica copies in other data centers will be used.
		- Though the system will be operational, clients may notice slowdown due to network latency. 
		  This is because multiple data centers are normally located at physically different locations and connected by a wide area network.

How is data written?
	Cassandra processes data at several stages on the write path, 
	starting with the immediate logging of a write and ending in with a write of data to disk:
		- Logging data in the commit log
		- Writing data to the memtable
		- Flushing data from the memtable
		- Storing data on disk in SSTables
		
	Logging writes and memtable storage
		When a write occurs, Cassandra stores the data in a memory structure called memtable, 
		and to provide configurable durability, it also appends writes to the commit log on disk. 
		The commit log receives every write made to a Cassandra node, and these durable writes survive permanently 
		even if power fails on a node. The memtable is a write-back cache of data partitions that Cassandra looks up by key. 
		The memtable stores writes in sorted order until reaching a configurable limit, and then is flushed.
		
	Flushing data from the memtable
		To flush the data, Cassandra writes the data to disk, in the memtable-sorted order. 
		A partition index is also created on the disk that maps the tokens to a location on disk. 
		When the memtable content exceeds the configurable threshold or the commitlog space exceeds the commitlog_total_space_in_mb, 
		the memtable is put in a queue that is flushed to disk. 
		The queue can be configured with the memtable_heap_space_in_mb or memtable_offheap_space_in_mb setting in the cassandra.yaml file. 
		If the data to be flushed exceeds the memtable_cleanup_threshold, Cassandra blocks writes until the next flush succeeds. 
		You can manually flush a table using nodetool flushor nodetool drain (flushes memtables without listening for connections to other nodes). 
		To reduce the commit log replay time, the recommended best practice is to flush the memtable before you restart the nodes. 
		If a node stops working, replaying the commit log restores to the memtable the writes that were there before it stopped.
		Data in the commit log is purged after its corresponding data in the memtable is flushed to an SSTable on disk.
		
	Storing data on disk in SSTables
		Memtables and SSTables are maintained per table. The commit log is shared among tables. 
		SSTables are immutable, not written to again after the memtable is flushed. 
		Consequently, a partition is typically stored across multiple SSTable files. 
		A number of other SSTable structures exist to assist read operations: [dml_write-process_12.png]
		
		For each SSTable, Cassandra creates these structures:
			Data (Data.db)
				The SSTable data

			Primary Index (Index.db)
				Index of the row keys with pointers to their positions in the data file

			Bloom filter (Filter.db)
				A structure stored in memory that checks if row data exists in the memtable before accessing SSTables on disk
			
			Compression Information (CompressionInfo.db)
				A file holding information about uncompressed data length, chunk offsets and other compression information

			Statistics (Statistics.db)
				Statistical metadata about the content of the SSTable

			Digest (Digest.crc32, Digest.adler32, Digest.sha1)
				A file holding adler32 checksum of the data file

			CRC (CRC.db)
				A file holding the CRC32 for chunks in an uncompressed file.

			SSTable Index Summary (SUMMARY.db)
				A sample of the partition index stored in memory

			SSTable Table of Contents (TOC.txt)
				A file that stores the list of all components for the SSTable TOC

			Secondary Index (SI_.*.db)
				Built-in secondary index. Multiple SIs may exist per SSTable


		The SSTables are files stored on disk. The naming convention for SSTable files has changed with Cassandra 2.2 and later to shorten the file path. 
		The data files are stored in a data directory that varies with installation. 
		For each keyspace, a directory within the data directory stores each table. 
		For example following path represents a data file. 
			/data/data/ks1/cf1-5be396077b811e3a3ab9dc4b9ac088d/la-1-big-Data.db 
		
		ks1 represents the keyspace name to distinguish the keyspace for streaming or bulk loading data. 
		A hexadecimal string, 5be396077b811e3a3ab9dc4b9ac088d in this example, is appended to table names to represent unique table IDs.
		Cassandra creates a subdirectory for each table, which allows you to symlink a table to a chosen physical drive or data volume. 
		This provides the capability to move very active tables to faster media, such as SSDs for better performance, 
		and also divides tables across all attached storage devices for better I/O balance at the storage layer.
		
Replication
	The data in each keyspace is replicated with a replication factor. 
	The most common replication factor used is three. 
	There is one primary replica of data that resides with the token owner node as explained in the data partitioning section. 
	The remainder of replicas is placed by Cassandra on specific nodes using the replica placement strategy. 
	All replicas are equally important for all database operations except for a few cluster mutation operations.

	There are two settings that mainly impact replica placement. 
		First is snitch, which determines the data center, and the rack a Cassandra node belongs to, and it is set at the node level. 
			They inform Cassandra about the network topology so that requests are routed efficiently and 
			allow Cassandra to distribute replicas by grouping machines into data centers and racks. 
			GossipingPropertyFileSnitch is the goto snitch for any cluster deployment. 
			It uses a configuration file called Cassandra-rackdc.properties on each node. 
			It contains the rack and data center name which hosts the node. 
			There is cloud-specific snitch available for AWS and GCP. 

		The second setting is the replication strategy. 
			The replication strategy is set at the keyspace level. 
			There are two strategies: 
				SimpleStrategy and NetworkTopologyStrategy. 
					The SimpleStrategy does not consider racks and multiple data centers. It places data replicas on nodes sequentially. 
					The NetworkTopologyStrategy is rack aware and data center aware. 
					SimpleStrategy should be only used for temporary and small cluster deployments, 
					for all other clusters NetworkTopologyStrategy is highly recommended. 
					A keyspace definition when used with NetworkTopologyStrategy specifies the number of replicas per data center as:
						cqlsh> create keyspace ks with replication = {'class' : 'NetworkTopologyStrategy', dc_1: 3, dc_2: 1}

Gossip protocol - failure detection
	When building a system on top of a set of wildly uncooperative and unruly computers you have knowledge problems: 
		knowing when other nodes are dead; 
		knowing when nodes become alive; 
		getting information about other nodes so you can make local decisions, 
		like knowing which node should handle a request based on a scheme for assigning nodes to a certain range of users; 
		learning about new configuration data; agreeing on data values; and so on.

	How do you solve these problems?
		A common centralized approach is to use a database and all nodes query it for information. 
		Obvious availability and performance issues for large distributed clusters. 
		Another approach is to use Paxos, a protocol for solving consensus in a network 
		to maintain strict consistency requirements for small groups of unreliable processes. 
		Not practical when larger number of nodes are involved.

	what's the super cool decentralized way to bring order to large clusters?
		Gossip protocols, which maintain relaxed consistency requirements amongst a very large group of nodes. 
		A gossip protocol is simple in concept. Each nodes sends out some data to a set of other nodes. 
		Data propagates through the system node by node like a virus. 
		Eventually data propagates to every node in the system. It's a way for nodes to build a global map from limited local interactions.

	A node only has to send to a subset of other nodes. That's it.

	In a distributed system you need at least two independent sources of information to mark a node down. 
	It's not enough to simply say because your node can't contact another node that the other node is down. 
	It's quite possible that your node is broken and the other node is fine. 
	But if other nodes in the system also see that other node is dead then you can with some confidence conclude that that node is dead. 
	Many complex hard to debug bugs are hidden here.

	Another advantage of this approach is data could flow directly into your monitoring system 
	rather than having a completely separate monitoring subsystem bolted on.

	There are 3 main types of gossip protocol:
		1. Dissemination Protocols :
				These protocols are also referred to as rumor-mongering protocols because they use gossip to spread information throughout the network, 
				they flood the members of the network with gossips in a way that produces the worst-case load.
		2. Anti-Entropy Protocols :
			These are used to repair the replicated data by comparing them and modifying the comparisons.
		3. Protocols that compute aggregates :
			These protocols work by or compute an aggregate of the network by sampling information at the nodes and 
			they combine the values to acquire a system-wide value – the largest value for some measurement nodes are making, smallest, etc.

Gossip protocol
	The Gossip protocol is used to repair the problems caused by multicasting; 
	it is a type of communication where a piece of information or gossip in this scenario, 
	is sent from one or more nodes to a set of other nodes in a network. 
	This is useful when a group of clients in the network require the same data at the same time. 
	But there are many problems that occur during multicasting, 
	if there are many nodes present at the recipient end, latency increases; the average time for a receiver to receive a multicast.
	
	To get this multicast message or gossip across the desired targets in the group, 
	the gossip protocol sends out the gossip periodically to random nodes in the network, 
	once a random node receives the gossip, it is said to be infected due to the gossip. 
	Now the random node that receives the gossip does the same thing as the sender, 
	it sends multiple copies of the gossip to random targets. This process continues until the target nodes get the multicast. 
	This process turns the infected nodes to uninfected nodes after sending the gossip out to random nodes.

	Gossip Dissemination
		Use random selection of nodes to pass on information to ensure it reaches all the nodes in the cluster without flooding the network.

	Problem
		In a cluster of nodes, each node needs to pass metadata information it has, 
		to all the other nodes in the cluster, without depending on a shared storage. 
		In a large cluster, if all servers communicate with all the other servers, 
		a lot of network bandwidth can be consumed. 
		Information should reach all the nodes even when some network links are experiencing issues.

	Solution
		Cluster nodes use gossip style communication to propagate state updates. 
		Each node selects a random node to pass the information it has. 
		This is done at a regular interval, say every 1 second. 
		Each time, a random node is selected to pass on the information.

	The Gossip protocol is the internal communication technique for nodes in a cluster to talk to each other. 
	Gossip is an efficient, lightweight, reliable inter-nodal broadcast protocol for diffusing data. 
	It's decentralized, "epidemic", fault tolerant and a peer-to-peer communication protocol. 
	Cassandra uses gossiping for peer discovery and metadata propagation.[gossip-ring]

	The gossip process runs every second for every node and exchange state messages with up to three other nodes in the cluster. 
	Since the whole process is decentralized, there is nothing or no one that coordinates each node to gossip. 
	Each node independently will always select one to three peers to gossip with. 
	It will always select a live peer (if any) in the cluster, it will probabilistically pick a seed node 
	from the cluster or maybe it will probabilistically select an unavailable node.[gossip-msg-exchange]

	The Gossip messaging is very similar to the TCP three-way handshake. 
	With a regular broadcast protocol, there could only have been one message per round, 
	and the data can be allowed to gradually spread through the cluster. 
	But with the gossip protocol, having three messages for each round of gossip adds a degree of anti-entropy. 
	This process allows obtaining "convergence" of data shared between the two interacting nodes much faster.

		SYN: The node initiating the round of gossip sends the SYN message which contains a compendium of the nodes in the cluster. 
		It contains tuples of the IP address of a node in the cluster, the generation and the heartbeat version of the node.

		ACK: The peer after receiving SYN message compares its own metadata information with the one sent by the initiator 
		and produces a diff. ACK contains two kinds of data. 
		One part consists of updated metadata information (AppStates) that the peer has but the initiator doesn't, 
		and the other part consists of digest of nodes the initiator has that the peer doesn't.

		ACK2: The initiator receives the ACK from peer and updates its metadata from the AppStates and 
		sends back ACK2 containing the metadata information the peer has requested for. 
		The peer receives ACK2, updates its metadata and the round of gossip concludes.

	An important note here is that this messaging protocol will cause only a constant amount of network traffic. 
	Since the broadcasting of the initial digest is limited to three nodes 
	and data convergence occurs through a pretty constant ACK and ACK2, 
	there will not be much of network spike. Although, if a node gets UP, all the nodes might want 
	to send data to that peer, causing the Gossip Storm.

	So how does a new node get the idea of whom to start gossiping with? 
		Well, Cassandra has many seed provider implementations that provide a list of seed addresses to the new node 
		and starts gossiping with one of them right away. After its first round of gossip, 
		it will now possess cluster membership information about all the other nodes in the cluster 
		and can then gossip with the rest of them.

	Well, how do we get to know if a node is UP/DOWN? 
		The Failure Detector is the only component inside Cassandra(only the primary gossip class can mark a node UP besides) to do so. 
		It is a heartbeat listener and marks down the timestamps and keeps backlogs of intervals 
		at which it receives heartbeat updates from each peer. 
		Based on the reported data, it determines whether a peer is UP/DOWN.

	How does a node being UP/DOWN affect the cluster? 
		The write operations stay unaffected. If a node does not get an acknowledgment for a write to a peer, 
		it simply stores it up as a hint. The nodes will stop sending read requests to a peer in DOWN state 
		and probabilistically gossiping can be tried upon since its an unavailable node, 
		as we have already discussed early on. 
		All repair, stream sessions are closed as well when an unavailable node is involved.

	What if a peer is responding very slowly or timing out? 
		Cassandra has another component called the Dynamic Snitch, which records and analyses 
		latencies of read requests to peer nodes. It ranks latencies of peers in a rolling window and 
		recalculates it every 100ms and resets the scores every 10mins to allow 
		for any other events(Eg: Garbage Collection) delaying the response time of a peer. 
		In this way, the Dynamic Snitch helps you identify the slow nodes and avoid them when indulging in Gossip.

Snitches
	In Cassandra Snitch is very useful and snitch is also helps in keep record to avoid storing multiple replicas of data on the same rack. 
	In Cassandra, it is very important aspects to avoid multiple replica. 
	In replication strategy we assign number of replica and also we define the data-center. 
	This information is very helpful for snitch to identify the node and which rack belong to. 

	In Cassandra, snitch job is to determine which data centers and racks it should use to read data from and write data to. 
	In Cassandra, all snitch are dynamic by default. 

	They inform Cassandra about the network topology so that requests are routed efficiently and 
	allows Cassandra to distribute replicas by grouping machines into datacenters and racks. 
	Specifically, the replication strategy places the replicas based on the information provided by the new snitch. 
	All nodes must return to the same rack and datacenter. 
	Cassandra does its best not to have more than one replica on the same rack (which is not necessarily a physical location).

	In cassandra, the snitch has two functions:
		1. it teaches Cassandra enough about your network topology to route requests efficiently.
		2. it allows Cassandra to spread replicas around your cluster to avoid correlated failures. 
		It does this by grouping machines into "datacenters" and "racks."

	Dynamic snitching
		The dynamic snitch monitor read latencies to avoid reading from hosts that have slowed down.
		Monitors the performance of reads from the various replicas and chooses the best replica based on this history.

	Snitch classes
		The endpoint_snitch parameter in cassandra.yaml should be set to the class that implements IEndPointSnitch 
		which will be wrapped by the dynamic snitch and decide if two endpoints are in the same data center or on the same rack. 
		Out of the box, Cassandra provides the snitch implementations:

		GossipingPropertyFileSnitch
			This should be your go-to snitch for production use. 
			The rack and datacenter for the local node are defined in cassandra-rackdc.properties 
			and propagated to other nodes via gossip. If cassandra-topology.properties exists, 
			it is used as a fallback, allowing migration from the PropertyFileSnitch.
			Automatically updates all nodes using gossip when adding new nodes and is recommended for production.
		
		SimpleSnitch
			Treats Strategy order as proximity. This can improve cache locality when disabling read repair. 
			Only appropriate for single-datacenter deployments.

		PropertyFileSnitch
			Proximity is determined by rack and data center, 
			which are explicitly configured in cassandra-topology.properties.

		Ec2Snitch
			Appropriate for EC2 deployments in a single Region, or in multiple regions 
			with inter-region VPC enabled (available since the end of 2017, see AWS announcement). 
			Loads Region and Availability Zone information from the EC2 API. 
			The Region is treated as the datacenter, and the Availability Zone as the rack. 
			Only private IPs are used, so this will work across multiple regions only if inter-region VPC is enabled.

		Ec2MultiRegionSnitch
			Uses public IPs as broadcast_address to allow cross-region connectivity (thus, 
			you should set seed addresses to the public IP as well). You will need to open the 
			storage_port or ssl_storage_port on the public IP firewall (For intra-Region traffic, 
			Cassandra will switch to the private IP after establishing a connection).

		RackInferringSnitch
			Proximity is determined by rack and data center, which are assumed to correspond 
			to the 3rd and 2nd octet of each node’s IP address, respectively. 
			Unless this happens to match your deployment conventions, 
			this is best used as an example of writing a custom Snitch class and is provided in that spirit.

		GoogleCloudSnitch
			Use the GoogleCloudSnitch for Cassandra deployments on Google Cloud Platform across one or more regions.

		CloudstackSnitch
			Use the CloudstackSnitch for Apache Cloudstack environments.

Data Distribution
	In Cassandra data distribution and replication go together. 
	In Cassandra distribution and replication depending on the three thing such that partition key, key value and Token range.

	Distribution provides power and resiliencwe
		One important Cassandra attribute is that its databases are distributed. 
		That yields both technical and business advantages. 
		Cassandra databases easily scale when an application is under high stress, 
		and the distribution also prevents data loss from any given datacenter’s hardware failure. 
		A distributed architecture also brings technical power; 
		for example, a developer can tweak the throughput of read queries or write queries in isolation.

		"Distributed" means that Cassandra can run on multiple machines while appearing to users as a unified whole. 
		There is little point in running Cassandsra as a single node, 
		although it is very helpful to do so to help you get up to speed on how it works. 
		But to get the maximum benefit out of Cassandra, you would run it on multiple machines.

		Since it is a distributed database, Cassandra can (and usually does) have multiple nodes. 
		A node represents a single instance of Cassandra. 
		These nodes communicate with one another through a protocol called gossip, 
		which is a process of computer peer-to-peer communication. 
		Cassandra also has a masterless architecture 
		– any node in the database can provide the exact same functionality as any other node – 
		contributing to Cassandra’s robustness and resilience. 
		Multiple nodes can be organized logically into a cluster, or "ring". You can also have multiple datacenters.

Staged event-driven architecture
	The staged event-driven architecture (SEDA) refers to an approach to software architecture that decomposes a complex, 
	event-driven application into a set of stages connected by queues. 
	It avoids the high overhead associated with thread-based concurrency models 
	(i.e. locking, unlocking, and polling for locks), and decouples event and thread scheduling from application logic. 
	By performing admission control on each event queue, the service can be well-conditioned to load, 
	preventing resources from being overcommitted when demand exceeds service capacity.
	
	SEDA employs dynamic control to automatically tune runtime parameters (such as the scheduling parameters of each stage) 
	as well as to manage load (like performing adaptive load shedding). 
	Decomposing services into a set of stages also enables modularity and code reuse, 
	as well as the development of debugging tools for complex event-driven applications.

	Let us consider a sample application which listens to a stream of messages in a Message Queue, 
	reads them and processes them based on some business logic, and sends Email. 
	A typical implementation of the above application uses multiple application-level threads to process messages. 
	A single thread will read a single message from the Message Queue and is responsible for processing a single message. 
	The horizontal scaling will be achieved by managing multiple processes deployed in multiple hosts.

	One of the problems with the logic for processing a message is organized as a single continuous flow of code 
	which makes the threads wait for any expensive IO operations involved in the business logic. 
	The impact of the expensive IO operations on the overall throughput of the system is barely noticeable for low scale systems. 
	But for the systems which need to process a million messages per second, 
	these expensive operations will introduce bottleneck for the overall throughput of the system.

	SEDA aims to resolve the above-mentioned problems by decomposing the application code into multiple stages, 
	and each stage communicates with each other using the queues.

Write Path Anatomy
	Coordinator
		When a request is sent to any Cassandra node, this node acts as a proxy for the application (actually, the Cassandra driver) 
		and the nodes involved in the request flow. This proxy node is called as the coordinator. 
		The coordinator is responsible for managing the entire request path and to respond back to the client.
		Besides, sometimes when the coordinator forwards a write request to the replica nodes, 
		they may happen to be unavailable at this very moment. In this case, the coordinator plays an important role 
		implementing a mechanism called Hinted Handoff.

	Partitioner
		Basically, for each node in the Cassandra cluster (Cassandra ring) is assigned a range of tokens.
		Cassandra distributes data across the cluster using a Consistent Hashing algorithm and, starting from version 1.2, 
		it also implements the concept of virtual nodes (vnodes), where each node owns a large number of small token ranges 
		in order to improve token reorganization and avoid hotspots in the cluster, that is, some nodes storing much more data than the others. Virtual nodes also allow to add and remove nodes in the cluster more easily and manages the token assignment automatically 
		for you so that you can enjoy a nice coffee when adding or removing a node instead of calculating and 
		assigning new token ranges for each node (which is a very error-prone operation, by the way).
		Well, that said, the partitioner is the component responsible for determining how to distribute the data across the nodes in the cluster given the partition key of a row. Basically, it is a hash function for computing a token given the partition key.
		Once the partitioner applies the hash function to the partition key and gets the token, 
		it knows exactly which node is going to handle the request.
	
	Replication
		Life would be much easier if…
			- Nodes never fail
			- Networks had no latency
			- People did not stumble on cables
			- Amazon did not restart your instances
			- Full GC meant “Full Guitar Concert”
		And so on. Unfortunately, these things happen all the time and you already chose a software engineer career.
		Fortunately, Cassandra offers automatic data replication and keeps your data redundant throughout different nodes in the cluster. 
		This means that (in certain levels) you can even resist to node failure scenarios and your data would still be safe and available. 
		But everything comes at a price, and the price of replication is consistency.

	Replication Strategy
		Basically, the coordinator uses the Replication Strategy to find out which nodes will be the replica nodes for a given request.
		There are two replication strategies available:
			- SimpleStrategy: used for a single data center deployment (not recommended for production environment). 
			  It doesn’t consider the network topology. Basically, it just takes the partitioner’s decision 
			  (that is, the node that will handle the request first based on the token range) 
			  and places the remaining replicas clockwise in relation to this node. 
			- NetworkTopologyStrategy: used for multiple data centers deployment (recommended for production environment). 
			  It also takes the partitioner’s decision and places the remaining replicas clockwise, 
			  but it also takes into consideration the rack and data centers configuration.

	Replication Factor
		When you create a table (Column Family) in Cassandra, you specify the replication factor. 
		The replication factor is the number of replicas that Cassandra will hold for this table in different nodes. 
		If you specify REPLICATION_FACTOR=3, then your data will be replicated to 3 different nodes throughout the cluster. 
		That provides fault tolerance and resilience because even if some nodes fail your data would still be safe and available.

	Write Consistency Level
		Do you still remember that when the client sends a request to a Cassandra node, 
		this node is called a coordinator and acts as a proxy between the client and the replica nodes?
		Well, when you write to a table in Cassandra (inserting data, for example), you can specify the write consistency level. 
		The write consistency level is the number of replica nodes that have to acknowledge the coordinator 
		that its local insert was successful (success here means that the data was appended to the commit log and written to the memtable). 
		As soon as the coordinator gets WRITE_CONSISTENCY_LEVEL success acknowledgments from the replica nodes, 
		it returns success back to the client and doesn’t wait for the remaining replicas to acknowledge success.
		For example, if an application issue an insert request with WRITE_CONSISTENCY_LEVEL=TWO to a table that is configured with REPLICATION_FACTOR=3, the coordinator will only return success to the application when two of the three replicas acknowledge success. 
		Of course, this doesn’t mean that the third replica will not write the data too; it will, but at this point, the coordinator would already have sent success back to the client.
		There are many different types of write consistency levels you can specify in your write request. 
		From the less consistent to full consistency: ANY, ONE, TWO, THREE, QUORUM, LOCAL_QUORUM, EACH_QUORUM, ALL.

	Write Flow Example
		For simplicity, suppose a write request is issued to a 6-node Cassandra cluster with the following characteristics:
			WRITE_CONSISTENCY_LEVEL=TWO
			TABLE_REPLICATION_FACTOR=3
			REPLICATION_STRATEGY=SimpleStrategy
		
		First, the client sends the write request to the Cassandra cluster using the driver. 
		The driver is responsible for a lot of features such as asynchronous IO, parallel execution, request pipelining, connection pooling, 
		auto node discovery, automatic reconnection, token awareness, and so on. 
		
		For example, by using a driver that implements a token-aware policy, the driver reduces network hops by sending requests 
		directly to the node that owns the data instead of sending it to a “random” coordinator.
		As soon as the coordinator gets the write request, it applies the partitioner hash function to the partition key 
		and uses the configured Replication Strategy in order to determine the TABLE_REPLICATION_FACTOR replica nodes 
		that will actually write the data (in this sentence, replace TABLE_REPLICATION_FACTOR with the number 3). 
		
		Now, before the coordinator forwards the write request to all the 3 replica nodes, 
		it will ask to the Failure Detector component how many of these replica nodes are actually available and compare it to the WRITE_CONSISTENCY_LEVEL provided in the request. If the number of replica nodes available is less than 
		the WRITE_CONSISTENCY_LEVEL provided, the Failure Detector will immediately throw an Exception.
		
		For our example, suppose the 3 replica nodes are available (that is, the Failure Detector will allow the request to continue). 
		Now, the coordinator will asynchronously forward the write request to all the replica nodes 
		(in these case, the 3 replica nodes that were figured in the first step). 
		As soon as WRITE_CONSISTENCY_LEVEL replica nodes acknowledge success (node2 and node4), 
		the coordinator returns success back to the driver.

		If the WRITE_CONSISTENCY_LEVEL for this request was THREE (or ALL), the coordinator would have to wait until 
		node3 acknowledges success too, and of course that this write request would be slower.
		So, basically…
			- Do you need fault tolerance and high availability? Use replication.
			- Just bear in mind that using replication means you will pay with consistency (for most of the cases, 
			  this is not a problem. Availability is often more important than consistency).
			- If consistency is not an issue for your domain, perfect. If it is, just increase the consistency level, 
			  but then you will pay with higher latency.
			- If you want fault tolerance and high availability, strong consistency and low latency, 
			  then you should be the client, not the software engineer (Lol).

	Hinted Handoff
		Suppose in the last example that only 2 of 3 replica nodes were available. In this case, the Failure Detector 
		would still allow the request to continue as the number of available replica nodes is not less than the WRITE_CONSISTENCY_LEVEL provided. 
		In this case, the coordinator would behave exactly as described before but there would be one additional step. 
		The coordinator would write locally the hint (the write request blob along with some metadata) in the disk (hints directory) 
		and would keep the hint there for 3 hours (by default) waiting for the replica node to become available again. 
		If the replica node recovers within this period, the coordinator will send the hint to the replica node so that 
		it can update itself and become consistent with the other replicas. If the replica node is offline for more than 3 hours, 
		then a read repair is needed. This process is referred as Hinted Handoff.

	Write Internals
		In short, when a write request reaches a node, mainly two things happen:
			1. The write request is appended to the commit log in the disk. This ensures data durability 
			   (the write request data would permanently survive even in a node failure scenario)
			2. The write request is sent to the memtable (a structure stored in the memory). When the memtable is full, 
			   the data is flushed to a SSTable on disk using sequential I/O and the data in the commit log is purged.

