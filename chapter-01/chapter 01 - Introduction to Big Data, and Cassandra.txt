What is Big Data?
	Big Data is a term used for a collection of data sets that are large and complex, 
	which is difficult to store and process using available database management tools or traditional data processing applications. 
	The challenge includes capturing, curating, storing, searching, sharing, transferring, analyzing and visualization of this data.

Big Data Characteristics
	The five characteristics (5V)- that define Big Data are: Volume, Velocity, Variety, Veracity and Value.

1. VOLUME
	Volume refers to the ‘amount of data’, which is growing day by day at a very fast pace. 
	The size of data generated by humans, machines and their interactions on social media itself is massive. 
	Researchers have predicted that 40 Zettabytes (40,000 Exabytes) will be generated by 2020, which is an increase of 300 times from 2005.

2. VELOCITY
	Velocity is defined as the pace at which different sources generate the data every day. This flow of data is massive and continuous. 
	There are 1.03 billion Daily Active Users (Facebook DAU) on Mobile as of now, which is an increase of 22% year-over-year. 
	This shows how fast the number of users are growing on social media and how fast the data is getting generated daily. 
	If you are able to handle the velocity, you will be able to generate insights and take decisions based on real-time data. 

3. VARIETY
	As there are many sources which are contributing to Big Data, the type of data they are generating is different. 
	It can be structured, semi-structured or unstructured. Hence, there is a variety of data which is getting generated every day. 
	Earlier, we used to get the data from excel and databases, now the data are coming in the form of images, audios, videos, sensor data etc. 
	Hence, this variety of unstructured data creates problems in capturing, storage, mining and analyzing the data.
	
	Types of Big Data
	Big Data could be of three types:

	1. Structured
		The data that can be stored and processed in a fixed format is called as Structured Data. 
		Data stored in a relational database management system (RDBMS) is one example of  ‘structured’ data. 
		It is easy to process structured data as it has a fixed schema. Structured Query Language (SQL) is often used to manage such kind of Data.
		
	2. Semi-Structured
		Semi-Structured Data is a type of data which does not have a formal structure of a data model, i.e. a table definition in a relational DBMS, 
		but nevertheless it has some organizational properties like tags and other markers to separate semantic elements that makes it easier to analyze. 
		XML files or JSON documents are examples of semi-structured data.
		
	3. Unstructured
		The data which have unknown form and cannot be stored in RDBMS and cannot be analyzed unless it is transformed into a structured format 
		is called as unstructured data. Text Files and multimedia contents like images, audios, videos are example of unstructured data. 
		The unstructured data is growing quicker than others, experts say that 80 percent of the data in an organization are unstructured. 
	
4. VERACITY
	Veracity refers to the data in doubt or uncertainty of data available due to data inconsistency and incompleteness. 
	In the image below, you can see that few values are missing in the table. Also, a few values are hard to accept, 
	for example – 15000 minimum value in the 3rd row, it is not possible. This inconsistency and incompleteness is Veracity.
	Data available can sometimes get messy and maybe difficult to trust. 
	With many forms of big data, quality and accuracy are difficult to control like Twitter posts with hashtags, abbreviations, typos and colloquial speech. 
	The volume is often the reason behind for the lack of quality and accuracy in the data. 

	Due to uncertainty of data, 1 in 3 business leaders don’t trust the information they use to make decisions.
	It was found in a survey that 27% of respondents were unsure of how much of their data was inaccurate.
	Poor data quality costs the US economy around $3.1 trillion a year.
	
5. VALUE
	After discussing Volume, Velocity, Variety and Veracity, there is another V that should be taken into account when looking at Big Data i.e. Value. 
	It is all well and good to have access to big data but unless we can turn it into value it is useless. 
	By turning it into value I mean, Is it adding to the benefits of the organizations who are analyzing big data? 
	Is the organization working on Big Data achieving high ROI (Return On Investment)? Unless, it adds to their profits by working on Big Data, it is useless.
	
Examples of Big Data
Daily we upload millions of bytes of data. 90 % of the world’s data has been created in last two years.

	* Walmart handles more than 1 million customer transactions every hour.
	* Facebook stores, accesses, and analyzes 30+ Petabytes of user generated data.
	* 230+ millions of tweets are created every day.
	* More than 5 billion people are calling, texting, tweeting and browsing on mobile phones worldwide.
	* YouTube users upload 48 hours of new video every minute of the day.
	* Amazon handles 15 million customer click stream user data per day to recommend products.
	* 294 billion emails are sent every day. Services analyses this data to find the spams.
	* Modern cars have close to 100 sensors which monitors fuel level, tire pressure etc. , each vehicle generates a lot of sensor data.
	
Applications of Big Data
	* Smarter Healthcare: Making use of the petabytes of patient’s data, the organization can extract meaningful information and then 
	build applications that can predict the patient’s deteriorating condition in advance.

	* Telecom: Telecom sectors collects information, analyzes it and provide solutions to different problems. By using Big Data applications, 
	telecom companies have been able to significantly reduce data packet loss, which occurs when networks are overloaded, and thus, 
	providing a seamless connection to their customers.

	* Retail: Retail has some of the tightest margins, and is one of the greatest beneficiaries of big data. 
	The beauty of using big data in retail is to understand consumer behavior. 
	Amazon’s recommendation engine provides suggestion based on the browsing history of the consumer.

	* Traffic control: Traffic congestion is a major challenge for many cities globally. 
	Effective use of data and sensors will be key to managing traffic better as cities become increasingly densely populated.

	* Manufacturing: Analyzing big data in the manufacturing industry can reduce component defects, improve product quality, 
	increase efficiency, and save time and money.

	* Search Quality: Every time we are extracting information from google, we are simultaneously generating data for it. 
	Google stores this data and uses it to improve its search quality.

Features of RDBMS Systems:
	* All data stored in the tables are provided by an RDBMS
	* Ensures that all data stored are in the form of rows and columns
	* Facilitates primary key, which helps in unique identification of the rows
	* Index creation for retrieving data at a higher speed
	* Facilitates a common column to be shared amid two or more tables
	* Multi-user accessibility is facilitated to be controlled by individual users
	* A virtual table creation is enabled to store sensitive data and simplify queries
	
Limitations for SQL database
	* Scalability: Users have to scale relational database on powerful servers that are expensive and difficult to handle. 
	To scale relational database it has 	to be distributed on to multiple servers. Handling tables across different servers is difficult .

	* Complexity: In SQL server’s data has to fit into tables anyhow. 
	If your data doesn’t fit into tables, then you need to design your database structure that will be complex and again difficult to handle.
	
NoSQL
	NoSQL commonly referred to as “Not Only SQL”. With NoSQL, unstructured ,schema less data can be stored in multiple collections 
	and nodes and it does not require fixed table sachems, it supports limited join queries , and we scale it horizontally.
	
Benefits of NoSQL
	highly and easily scalable
		Relational database or RDBMS databases are vertically Scalable When load increase on RDBMS database then 
		we scale database by increasing server hardware power ,need to by expensive and bigger servers and NoSQL databases are designed 
		to expand horizontally and in Horizontal scaling means that you scale by adding more machines into your pool of resources.
		
	Maintaining NoSQL Servers is Less Expensive
		Maintaining high-end RDBMS systems is expensive and need trained manpower for database management but NoSQL databases require less management. 
		it support many Features like automatic repair, easier data distribution, and simpler data models make administration 
		and tuning requirements lesser in NoSQL.
		
	Lesser Server Cost and open-Source
		NoSQL databases are cheap and open source. NoSql database implementation is easy and typically uses cheap servers 
		to manage the exploding data and transaction while RDBMS databases are expensive and it uses big servers and storage systems. 
		So the storing and processing data cost per gigabyte in the case of NoSQL can be many times lesser than the cost of RDBMS.
		
	No Schema or Fixed Data model
		NoSQL database is schema less so Data can be inserted in a NoSQL database without any predefined schema. 
		So the format or data model can be changed any time, without application disruption.and change management is a big headache in SQL.
		
	Support Integrated Caching
		NoSQL database support caching in system memory so it increase data output performance and SQL database 
		where this has to be done using separate infrastructure.

Limitations & disadvantage of NoSQL
	1. Lack of standardizing. NoSQL database is Open Source and Open Source at its greatest strength but 
		at the same time its greatest weakness because there are not many defined standards for NoSQL databases, so no two NoSQL databases are equal
	2. No Stored Procedures in mongodb (NoSql database).
	3. GUI mode tools to access the database is not flexibly available in market. They usually have not-really-useful management tools or console access.
	4. Too difficult for finding nosql experts because it is latest technology and NoSQL developer are in learning mode
	5. Not all NoSQL databases contemplate the atomicity of instructions and the integrity of the data. They withstand what’s know as eventual consistence.
	

NoSQL vs SQL: When should we use which type of database?
	1. When the data must be consisten without leaving room for error when using a relational database. SQL.
	2. When our budget won’t allow large devices and must be put into lower performance devices. NoSQL.
	3. When the datastructures we manage are variable. NoSQL.
	4. For analyzing large quantities of data in read mode only. NoSQL.
	5. Event capture and processing. NoSQL
	6. Online stores with complex intelligence engines. NoSQL

RDBMS
	Relational Databases strictly comply with ACID guarantees and hence are a good choice for transactional data. 
	Relational Databases are mainly based on a single node design. 
	That said, lately, most of the popular ones have been adding cluster support through the use of Sharding. 
	But none of them are as elegant as their multi-node NoSQL counterparts since partition tolerance is built into their foundation. 
	Sharding leads to increased costs and requires close management.
	
	It is safe to say that most Relational Databases prioritize consistency and availability over partition tolerance. 
	This means Relational Databases are not very good at handling a large amount of data since the data they have to be 
	stored in a single system or need constant babysitting because of their limitations when it comes to multi-node operation.
	
NoSQL
	NoSQL Databases are great at storing semi-structured or non-structured data since they don’t enforce a concrete schema for tables. 
	This means data attributes can be added on the fly without changing the structure of the entire table or adding redundant elements to the rest of the rows. 
	Since there is no particular structure enforced by the database, they are also not very good at join queries. 
	NoSQL Databases recommend data to be stored in a format in which it will be frequently accessed. 
	This helps in defining the database very close to the UI layer or where the data will be actually used or reported.
	
	NoSQL Databases are great at scaling horizontally and partition tolerance is built into their foundation. 
	NoSQL Databases do well in scenarios where sub-second response time for high data volume is required. 
	NoSQL Databases achieve this by compromising consistency and referential integrity. 
	Most NoSQL Databases support only eventual consistency and are hence not a great choice for transactional operations. 
	Lately, databases like MongoDB have found some success in breaking this barrier. Here is the article on MongoDB vs MySQL.
	Article Link: https://hevodata.com/learn/mongodb-vs-mysql-7-critical-differences/
	
	NoSQL is an umbrella term to describe a whole set of databases that do not conform to the structured data format. It consists of the following:

		1. Document Databases
			Document Databases store data as objects in JSON form.
		2. Key-Value-Based Databases 
			Key-Value-based Databases store data as a collection of key-value pairs.
		3. Column-Oriented Databases
			Column-Oriented Databases store data as a collection of columns and perform great when specific columns are accessed.
		4. Graph Databases
			Graph Databases store data as nodes and relationships. 
			They help users to express complex relationships that exist between data elements and query them using specialized Graph Query Languages.
			
Factors that Drive the Relational Database Vs NoSQL Decision
	1. Schema Flexibility
		* The biggest advantage offered by NoSQL Databases is the flexibility of schema.
		* So the very question that you should ask yourself is whether your use case can take advantage of this schema flexibility.
		* IoT platform that stores data from different kinds of sensors --> NoSQL
		* Simple web application with all the user attributes known upfront --> RDBMS
	
	2. Workload Volume
		* Due to the distributed storage and processng NoSQL most of the time sbetter choice to large volume of data
		* Another critical factor is minimum hardware level to perform acceptably. NoSQL can be run and operate in commodity (inexpensive) hardware
	
	3. Data Consistency
		Relational Databases are great at enforcing consistency. 
		NoSQL Databases mostly go by eventual consistency when it comes to writes. 
		This means, there is a chance that your application will read old data till the time writes are propagated to all the nodes. 
		If your application cannot afford such scenarios, you should use a classic Relational Database. 
		This limitation of NoSQL Databases makes them a non-starter for transactional loads.
	
	4. Write Performance Requirements
		NoSQL Databases compromise consistency to achieve fast write performance. 
		SQL databases offer to write safely with consistency but at the expense of a bit of speed. 
		Eventual consistency may be a strict nonstarter in some use cases but may be acceptable in others.

		A good answer to the question ‘Can we afford to let go of strict consistency for faster writes ?’ 
			can help you arrive at the RDBMS vs NoSQL decision quickly.
	
	5. Read Requirements
		RDBMS possesses a great ability to query data and execute complex joins. 
		NoSQL Databases perform best when data is stored in the same form in which they are to be consumed. 

		For example, let’s say you are creating a reporting solution. 
		You can choose to implement it by storing data for specific reports in different tables and access it through a simple select statement, 
		in this case, you are better off with a NoSQL Database. 
		The other choice is to store the base data in a small number of related tables and execute various queries 
		and aggregate them to form different reports and this use case points to using a Relational Database
	
	6. Infrastructure Constraints
		NoSQL Databases are well known for their ability to run using cheap general-purpose hardware and scale horizontally. 
		Since the cost of a high-end special-purpose instance is more than multiple cheap general-purpose instances, 
		there is the possibility of cost advantage in case you use a NoSQL Database. 
		This becomes valid only when your data volume is significant enough for a distributed database to make sense. 
		For handling TBs of data, Relational Databases often require high-end special-purpose hardware.
		
Common characteristics of NoSQL databases
	* They have higher scalability.
	* They use distributed computing.
	* They are cost effective.
	* They support flexible schema.
	* They can process both unstructured and semi-structured data.
	* There are no complex relationships, such as the ones between tables in an RDBMS.
	
CAP theorem
	In theoretical computer science, the CAP theorem, also named Brewer's theorem after computer scientist Eric Brewer, 
	states that any distributed data store can only provide two of the following three guarantees:

	1. Consistency
		Every read receives the most recent write or an error.

	2. Availability
		Every request receives a (non-error) response, without the guarantee that it contains the most recent write.

	3. Partition tolerance
		The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes.
		When a network partition failure happens, it must be decided whether to cancel the operation and thus decrease the availability 
		but ensure consistency or to proceed with the operation and thus provide availability but risk inconsistency.
		
	Thus, if there is a network partition, one has to choose between consistency and availability. 
	Note that consistency as defined in the CAP theorem is quite different from the consistency guaranteed in ACID database transactions.
	Eric Brewer argues that the often-used "two out of three" concept can be somewhat misleading because system designers 
	only need to sacrifice consistency or availability in the presence of partitions, but that in many systems partitions are rare.
	
	No distributed system is safe from network failures, thus network partitioning generally has to be tolerated. 
	In the presence of a partition, one is then left with two options: consistency or availability. 
	When choosing consistency over availability, the system will return an error or a time out 
	if particular information cannot be guaranteed to be up to date due to network partitioning. 
	When choosing availability over consistency, the system will always process the query and 
	try to return the most recent available version of the information, even if it cannot guarantee it is up to date due to network partitioning.
	
	CAP is often misunderstood as a choice at all times of which one of the three guarantees to abandon. 
	In fact, the choice is between consistency and availability only when a network partition or failure happens. 
	When there is no network failure, both availability and consistency can be satisfied.

When it’s a ‘yes’
	In combination with Apache Spark and the like, Cassandra can be a strong ‘backbone’ for real-time analytics. 
	And it scales linearly. So, if you anticipate growth of your real-time data, Cassandra definitely has the utmost advantage here.

When it’s a ‘yes’
	ACID transactions
		core banking systems handling bank transfers, among other things
	
	Strong consistency
		To achieve high availability, Cassandra sacrifices strong consistency and only grants eventual one.
	
	Lots of updates and deletes
		 Cassandra is incredible at writes (here are the reasons for this amazing write performance). 
		 But it’s only append-oriented. If you need to update a lot, Cassandra’s no good.
		 
	Lots of scans
		Cassandra reads data pretty well. But it’s good at reading as long as you know the primary key of data you want. 
		If you don’t, Cassandra will have to scan all nodes to find what you need, which will take a while. 
		And if the latency threshold is exceeded, the scan will not be completed at all.
		
Real use cases when it’s a ‘sure’
	Sensor data
		The way Cassandra’s data model is organized and the fact that Cassandra is designed for intensive write workloads 
		make it exceptionally good for sensor data. It suits completely different industries, be it manufacturing, logistics, 
		healthcare, real estate, energy production, agriculture or whatever. 
		Regardless of sensor types, Cassandra handles the flow of incoming data nicely and provides possibilities for further data analysis.
	
	Messaging systems
		Messaging systems (chats, collaboration and instant messaging apps, etc.) are just as perfect for Cassandra as sensor data, 
		since they don’t require data updates. Cassandra quickly writes new incoming messages, allows quick reads and other additional features. For instance, 
		you may give a message a ‘time to live’ and Cassandra will delete it when this time runs out avoiding expensive tombstones and compaction.
		
	Ecommerce websites
		Data model design, write-orientation, fairly fast reads and linear scalability make Cassandra suitable for ecommerce websites with features 
		like product catalogs and recommendation or personalization engines. For the latter, Cassandra can store activities of visitors, 
		who fall into the same segment, close to each other, which will allow analytical tools a quick access to the data to, 
		say, generate tempting recommendations for users who want to leave the website.
		
	Entertainment websites
		Cassandra also helps various entertainment websites track and monitor their users’ activities. 
		It stores data on what movies, games, articles or songs a user has watched, played, read or listened to, 
		how much time they spent on each activity, etc. Then, Cassandra can feed this data to an analytical tool to recommend 
		other movies, games, articles or songs users may like.
		
	Fraud detection for banks
		Although Cassandra doesn’t go well with transfers between bank accounts and poorly gets along with ACID transactions, 
		banks still can benefit from it. Their big data solutions built to analyze customer data can provide an extra level of security 
		for their clients by enabling fraud detection. Cassandra does it splendidly, given its great speeds and 
		support for real-time analytics through a seamless integration with Apache Spark.
		
	